================================================================================
üî• –í–ê–® –ü–†–û–ï–ö–¢ - –°–¢–†–£–ö–¢–£–†–ê –¢–ê –§–ê–ô–õ–ò
üìÖ 2026-02-19 21:34:40
================================================================================

üìÅ –°–¢–†–£–ö–¢–£–†–ê –í–ê–®–û–ì–û –ü–†–û–ï–ö–¢–£:
--------------------------------------------------------------------------------

üìÅ pro_scraper_stealth_project3/
  üìÑ üîí .gitignore (GITIGNORE)
  üìÑ combine_project.py
  üìÑ ‚öôÔ∏è config.yaml (CONFIG)
  üìÑ main.py
  üìÑ README.md
  üìÑ requirements.txt
  üìÅ .idea/
    üìÑ üîí .gitignore (GITIGNORE)
  üìÅ .venv/
    üìÑ üîí .gitignore (GITIGNORE)
  üìÅ src/
    üìÑ __init__.py
    üìÑ client.py
    üìÑ exporter.py
    üìÑ models.py
    üìÑ parser.py
    üìÑ proxy_fetcher.py
    üìÑ proxy_monitor.py
    üìÑ proxy_utils.py
    üìÑ scraper.py
    üìÑ semaphore_manager.py
    üìÑ settings.py
    üìÑ state_manager.py
    üìÑ stealth.py
    üìÑ stealth_config.py
    üìÑ utils.py

================================================================================

üìÑ –í–ú–Ü–°–¢ –í–ê–®–ò–• –§–ê–ô–õ–Ü–í:
================================================================================


================================================================================
üìÑ üîí –§–ê–ô–õ: .gitignore (GITIGNORE)
================================================================================

# Python (—Ç–∏–º—á–∞—Å–æ–≤—ñ —Ñ–∞–π–ª–∏ –∫–æ–º–ø—ñ–ª—è—Ü—ñ—ó)
__pycache__/
*.py[cod]
*$py.class

# –í—ñ—Ä—Ç—É–∞–ª—å–Ω–µ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ (—Ü–µ –∑–∞–º–æ–≤–Ω–∏–∫ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–∞–º —á–µ—Ä–µ–∑ requirements.txt)
.venv/
venv/
ENV/

# –ö–æ–Ω—Ñ—ñ–¥–µ–Ω—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ (—Ç–≤–æ—è —Å–µ—Å—ñ—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü—ñ—ó)
auth.json

# –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É —Ç–∞ –ª–æ–≥–∏
data/*.csv
data/*.json
data/*.log
!data/.gitkeep

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è IDE (PyCharm, VS Code)
.idea/
.vscode/

# –°–∏—Å—Ç–µ–º–Ω—ñ —Ñ–∞–π–ª–∏
.DS_Store
Thumbs.db

config.yaml

.env
*.env
data/valid_proxies.json
__pycache__/

================================================================================
üìÑ üîí –§–ê–ô–õ: .idea\.gitignore (GITIGNORE)
================================================================================

# Default ignored files
/shelf/
/workspace.xml
# Editor-based HTTP Client requests
/httpRequests/

================================================================================
üìÑ üîí –§–ê–ô–õ: .venv\.gitignore (GITIGNORE)
================================================================================

# created by virtualenv automatically
*

================================================================================
üìÑ –§–ê–ô–õ: combine_project.py
================================================================================

# dump_my_structure.py
"""
–°—Ç–≤–æ—Ä—é—î –¥–∞–º–ø –∑—ñ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é –ø—Ä–æ–µ–∫—Ç—É —ñ –≤–º—ñ—Å—Ç–æ–º —Ç—ñ–ª—å–∫–∏ –≤–∞—à–∏—Ö —Ñ–∞–π–ª—ñ–≤
–í–∏–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é
"""

import os
from pathlib import Path
from datetime import datetime


def dump_my_structure():
    root_dir = Path(__file__).parent
    output_file = root_dir / "my_project_structure.txt"

    print(f"üìÅ –°—Ç–≤–æ—Ä—é—é –¥–∞–º–ø —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –≤–∞—à–æ–≥–æ –ø—Ä–æ–µ–∫—Ç—É: {output_file}")
    print(f"üìÇ –†–æ–±–æ—á–∞ –ø–∞–ø–∫–∞: {root_dir}")

    # –¢–Ü–õ–¨–ö–ò –≤–∞—à—ñ —Ñ–∞–π–ª–∏ - —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è, —è–∫—ñ –≤–∏ —Å—Ç–≤–æ—Ä—é–≤–∞–ª–∏
    extensions = {'.py', '.yaml', '.yml', '.env', '.md', '.txt', '.json'}

    # –ü–∞–ø–∫–∏, —è–∫—ñ –¢–†–ï–ë–ê –≤–∫–ª—é—á–∏—Ç–∏ (–≤–∞—à –∫–æ–¥)
    include_dirs = {'src', 'test'}

    # –ü–∞–ø–∫–∏, —è–∫—ñ –¢–†–ï–ë–ê –≤–∏–∫–ª—é—á–∏—Ç–∏
    exclude_dirs = {
        '__pycache__', '.venv', 'venv', 'env', '.git', '.idea',
        'node_modules', 'dist', 'build', '.pytest_cache', 'data', 'logs'
    }

    # –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —á–∏ —Ñ–∞–π–ª –≤–∞—à
    def is_my_file(path):
        rel_path = path.relative_to(root_dir)
        parts = rel_path.parts

        # ‚ö†Ô∏è –°–ü–ï–¶–Ü–ê–õ–¨–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê –î–õ–Ø .gitignore
        if path.name == '.gitignore':
            return True

        # –í–∏–∫–ª—é—á–∞—î–º–æ —Å–∏—Å—Ç–µ–º–Ω—ñ –ø–∞–ø–∫–∏
        if any(excl in str(rel_path) for excl in exclude_dirs):
            return False

        # –Ø–∫—â–æ —Ñ–∞–π–ª –≤ src –∞–±–æ test - –±–µ—Ä–µ–º–æ
        if len(parts) > 0 and parts[0] in include_dirs:
            return True

        # –§–∞–π–ª–∏ –≤ –∫–æ—Ä–µ–Ω—ñ –ø—Ä–æ–µ–∫—Ç—É
        if len(parts) == 1:
            # –í–∞–∂–ª–∏–≤—ñ —Ñ–∞–π–ª–∏ –≤ –∫–æ—Ä–µ–Ω—ñ
            if path.name in ['main.py', 'config.yaml', '.env', 'requirements.txt',
                             'README.md', 'dump_my_structure.py', 'combine_project.py']:
                return True
            # –§–∞–π–ª–∏ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è–º
            if path.suffix in extensions:
                return True

        return False

    # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –≤–∞—à—ñ —Ñ–∞–π–ª–∏ —ñ –ø–∞–ø–∫–∏
    my_files = []
    my_dirs = set()

    print("\nüìÇ –ó–ë–Ü–† –§–ê–ô–õ–Ü–í:")
    for path in sorted(root_dir.rglob('*')):
        if path.is_file() and is_my_file(path):
            my_files.append(path)
            print(f"   + {path.relative_to(root_dir)}")
            # –î–æ–¥–∞—î–º–æ –≤—Å—ñ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫—ñ –ø–∞–ø–∫–∏
            parent = path.parent
            while parent != root_dir:
                my_dirs.add(parent)
                parent = parent.parent

    # –°–æ—Ä—Ç—É—î–º–æ
    my_files.sort()
    my_dirs = sorted(my_dirs)

    # –í—ñ–¥–∫—Ä–∏–≤–∞—î–º–æ —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å—É
    with open(output_file, 'w', encoding='utf-8') as out:
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        out.write("=" * 80 + "\n")
        out.write("üî• –í–ê–® –ü–†–û–ï–ö–¢ - –°–¢–†–£–ö–¢–£–†–ê –¢–ê –§–ê–ô–õ–ò\n")
        out.write(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        out.write("=" * 80 + "\n\n")

        # ===== –°–¢–†–£–ö–¢–£–†–ê =====
        out.write("üìÅ –°–¢–†–£–ö–¢–£–†–ê –í–ê–®–û–ì–û –ü–†–û–ï–ö–¢–£:\n")
        out.write("-" * 80 + "\n\n")

        def print_structure(dir_path, level=0):
            """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤–∏–≤–æ–¥–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –±–µ–∑ –¥—É–±–ª—é–≤–∞–Ω–Ω—è"""
            indent = "  " * level

            # –í–∏–≤–æ–¥–∏–º–æ –Ω–∞–∑–≤—É –ø–∞–ø–∫–∏
            if level == 0:
                out.write(f"{indent}üìÅ {root_dir.name}/\n")
            else:
                out.write(f"{indent}üìÅ {dir_path.name}/\n")

            # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –ø—ñ–¥–ø–∞–ø–∫–∏ —ñ —Ñ–∞–π–ª–∏ –≤ —Ü—ñ–π –ø–∞–ø—Ü—ñ
            items = []
            subdirs = []

            # –°–ø–æ—á–∞—Ç–∫—É –∑–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –ø—ñ–¥–ø–∞–ø–∫–∏
            for path in sorted(dir_path.iterdir()):
                if path.is_dir():
                    if path in my_dirs or any(f.parent == path for f in my_files):
                        subdirs.append(('dir', path))

            # –ü–æ—Ç—ñ–º –∑–±–∏—Ä–∞—î–º–æ —Ñ–∞–π–ª–∏ –≤ –ø–æ—Ç–æ—á–Ω—ñ–π –ø–∞–ø—Ü—ñ
            files = []
            for path in sorted(dir_path.iterdir()):
                if path.is_file() and path in my_files:
                    files.append(('file', path))

            # –°–ø–æ—á–∞—Ç–∫—É –≤–∏–≤–æ–¥–∏–º–æ —Ñ–∞–π–ª–∏
            for item_type, item_path in files:
                if item_path.name == '.gitignore':
                    out.write(f"{indent}  üìÑ üîí {item_path.name} (GITIGNORE)\n")
                elif item_path.name == '.env':
                    out.write(f"{indent}  üìÑ üîê {item_path.name} (ENV)\n")
                elif item_path.name == 'config.yaml':
                    out.write(f"{indent}  üìÑ ‚öôÔ∏è {item_path.name} (CONFIG)\n")
                else:
                    out.write(f"{indent}  üìÑ {item_path.name}\n")

            # –ü–æ—Ç—ñ–º –≤–∏–≤–æ–¥–∏–º–æ –ø—ñ–¥–ø–∞–ø–∫–∏ (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ)
            for item_type, item_path in subdirs:
                print_structure(item_path, level + 1)

        # –ü–æ—á–∏–Ω–∞—î–º–æ –∑ –∫–æ—Ä–µ–Ω—è
        print_structure(root_dir)

        # ===== –í–ú–Ü–°–¢ =====
        out.write("\n" + "=" * 80 + "\n\n")
        out.write("üìÑ –í–ú–Ü–°–¢ –í–ê–®–ò–• –§–ê–ô–õ–Ü–í:\n")
        out.write("=" * 80 + "\n\n")

        for path in my_files:
            rel_path = path.relative_to(root_dir)
            out.write(f"\n{'=' * 80}\n")

            if path.name == '.gitignore':
                out.write(f"üìÑ üîí –§–ê–ô–õ: {rel_path} (GITIGNORE)\n")
            elif path.name == '.env':
                out.write(f"üìÑ üîê –§–ê–ô–õ: {rel_path} (ENV)\n")
            elif path.name == 'config.yaml':
                out.write(f"üìÑ ‚öôÔ∏è –§–ê–ô–õ: {rel_path} (CONFIG)\n")
            else:
                out.write(f"üìÑ –§–ê–ô–õ: {rel_path}\n")

            out.write(f"{'=' * 80}\n\n")
            try:
                content = path.read_text(encoding='utf-8')
                out.write(content)
                if not content.endswith('\n'):
                    out.write('\n')
            except Exception as e:
                out.write(f"[–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è: {e}]\n")

        # ===== –°–¢–ê–¢–ò–°–¢–ò–ö–ê =====
        out.write("\n" + "=" * 80 + "\n")
        out.write("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
        out.write("=" * 80 + "\n")
        out.write(f"üìÅ –ü–∞–ø–æ–∫ –∑ –∫–æ–¥–æ–º: {len(my_dirs)}\n")
        out.write(f"üìÑ –í—Å—å–æ–≥–æ —Ñ–∞–π–ª—ñ–≤: {len(my_files)}\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞—Ö
        stats = {}
        for f in my_files:
            ext = f.suffix or '(–±–µ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è)'
            stats[ext] = stats.get(ext, 0) + 1

        out.write("\nüìä –ü–æ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è—Ö:\n")
        for ext, count in sorted(stats.items()):
            out.write(f"  {ext}: {count} —Ñ–∞–π–ª—ñ–≤\n")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞–ø–∫–∞—Ö
        out.write("\nüìä –ü–æ –ø–∞–ø–∫–∞—Ö:\n")
        dir_stats = {}
        for f in my_files:
            parent = f.parent.name or 'root'
            dir_stats[parent] = dir_stats.get(parent, 0) + 1

        for dir_name, count in sorted(dir_stats.items()):
            out.write(f"  üìÅ {dir_name}: {count} —Ñ–∞–π–ª—ñ–≤\n")

        gitignore_count = len([f for f in my_files if f.name == '.gitignore'])
        out.write(f"\nüîí .gitignore: {'–Ñ' if gitignore_count > 0 else '–ù–ï–ú–ê–Ñ'}\n")
        out.write("=" * 80 + "\n")

    print(f"\n‚úÖ –ì–û–¢–û–í–û! –§–∞–π–ª: {output_file}")
    print(f"üìä –†–æ–∑–º—ñ—Ä: {output_file.stat().st_size / 1024:.1f} KB")

    # –ü—ñ–¥—Å—É–º–æ–∫
    gitignore_in_dump = any(f.name == '.gitignore' for f in my_files)
    if gitignore_in_dump:
        print(f"üîí .gitignore –£–°–ü–Ü–®–ù–û –í–ö–õ–Æ–ß–ï–ù–û!")
    else:
        print(f"‚ùå .gitignore –ù–ï –í–ö–õ–Æ–ß–ï–ù–û!")


if __name__ == "__main__":
    dump_my_structure()

================================================================================
üìÑ ‚öôÔ∏è –§–ê–ô–õ: config.yaml (CONFIG)
================================================================================

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É
scraping:
  start_url: "https://books.toscrape.com"
  max_items: 50
  concurrency: 1

# –ü–∞—É–∑–∏ –º—ñ–∂ –¥—ñ—è–º–∏ (—Å–µ–∫—É–Ω–¥–∏)
delays:
  min: 2
  max: 5

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±—Ä–∞—É–∑–µ—Ä–∞
browser:
  headless: false
  timeout: 60000

# –°–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è books.toscrape.com
selectors:
  book_card: "article.product_pod"
  title: "h3 a"
  price: "p.price_color"
  availability: ".instock.availability"
  rating: "p.star-rating"
  image: "div.image_container img"
  next_button: "li.next a"

# –ü—Ä–æ–∫—Å—ñ (–≤–∞—à—ñ –¥–∞–Ω—ñ)
proxies:
  - server: "http://31.59.20.176:6754"
    username: "uaeomrhe"
    password: "q5e8bmkvkeox"
  # ... —Ä–µ—à—Ç–∞ –ø—Ä–æ–∫—Å—ñ

================================================================================
üìÑ –§–ê–ô–õ: main.py
================================================================================

# main.py
import asyncio
import sys
import argparse
from pathlib import Path
from loguru import logger
from datetime import datetime

# –î–æ–¥–∞—î–º–æ —à–ª—è—Ö –¥–æ –ø—Ä–æ–µ–∫—Ç—É
sys.path.append(str(Path(__file__).parent))

from src.scraper import Scraper
from src.exporter import Exporter
from src.settings import LOG_DIR, MAX_ITEMS, VALID_PROXY_LIST
from src.stealth import get_stealth_for_site


def setup_logging():
    """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
    logger.remove()

    # –ö–æ–Ω—Å–æ–ª—å–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è (–∫–æ–ª—å–æ—Ä–æ–≤–µ)
    logger.add(
        sys.stdout,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        level="INFO",
        colorize=True
    )

    # –§–∞–π–ª–æ–≤–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è (–¥–µ—Ç–∞–ª—å–Ω–µ)
    log_file = LOG_DIR / f"olx_scraper_{datetime.now().strftime('%Y%m%d')}.log"
    logger.add(
        log_file,
        rotation="10 MB",
        retention="10 days",
        level="DEBUG",
        encoding="utf-8",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"
    )

    return log_file


def parse_arguments():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞"""
    parser = argparse.ArgumentParser(description="–°–∫—Ä–∞–ø–µ—Ä –¥–ª—è OLX.ua")

    parser.add_argument(
        '--max-items',
        type=int,
        default=MAX_ITEMS,
        help=f'–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–≥–æ–ª–æ—à–µ–Ω—å (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: {MAX_ITEMS})'
    )

    parser.add_argument(
        '--concurrent',
        type=int,
        default=3,
        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 3)'
    )

    parser.add_argument(
        '--headless',
        action='store_true',
        help='–ó–∞–ø—É—Å–∫ –≤ headless —Ä–µ–∂–∏–º—ñ (–±–µ–∑ –≤—ñ–∫–Ω–∞ –±—Ä–∞—É–∑–µ—Ä–∞)'
    )

    parser.add_argument(
        '--no-proxy',
        action='store_true',
        help='–í–∏–º–∫–Ω—É—Ç–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è)'
    )

    parser.add_argument(
        '--format',
        type=str,
        default='csv',
        choices=['csv', 'json', 'both'],
        help='–§–æ—Ä–º–∞—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ (csv, json, both)'
    )

    parser.add_argument(
        '--delay',
        type=float,
        default=3.0,
        help='–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö'
    )

    parser.add_argument(
        '--query',
        type=str,
        default='–Ω–æ—É—Ç–±—É–∫',
        help='–ü–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: –Ω–æ—É—Ç–±—É–∫, —Å–º–∞—Ä—Ç—Ñ–æ–Ω, –∞–≤—Ç–æ)'
    )

    parser.add_argument(
        '--city',
        type=str,
        default='',
        help='–ú—ñ—Å—Ç–æ –¥–ª—è –ø–æ—à—É–∫—É (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥: –∫–∏—ó–≤, –ª—å–≤—ñ–≤, —Ö–∞—Ä–∫—ñ–≤)'
    )

    parser.add_argument(
        '--debug',
        action='store_true',
        help='–†–µ–∂–∏–º –Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è (–¥–µ—Ç–∞–ª—å–Ω—ñ –ª–æ–≥–∏)'
    )

    return parser.parse_args()


def build_olx_url(query: str, city: str = "") -> str:
    """
    –§–æ—Ä–º—É—î URL –¥–ª—è OLX –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ—à—É–∫–æ–≤–æ–≥–æ –∑–∞–ø–∏—Ç—É

    Args:
        query: –ü–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç
        city: –ú—ñ—Å—Ç–æ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        str: URL –¥–ª—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É
    """
    base_url = "https://www.olx.ua/uk/list"

    if city:
        # –î–æ–¥–∞—î–º–æ –º—ñ—Å—Ç–æ –¥–æ URL
        return f"{base_url}/q-{query}/?search%5Bcity%5D={city}"
    else:
        return f"{base_url}/q-{query}/"


async def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –∑–∞–ø—É—Å–∫—É —Å–∫—Ä–∞–ø–µ—Ä–∞ –¥–ª—è OLX.UA"""

    # 1. –ü–∞—Ä—Å–∏–º–æ –∞—Ä–≥—É–º–µ–Ω—Ç–∏ –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞
    args = parse_arguments()

    # 2. –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è
    log_file = setup_logging()

    # 3. –§–æ—Ä–º—É—î–º–æ URL –¥–ª—è –ø–æ—à—É–∫—É
    search_url = build_olx_url(args.query, args.city)

    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –°–ö–†–ê–ü–ï–†–ê –î–õ–Ø OLX.UA")
    logger.info("=" * 60)
    logger.info(f"üîç –ü–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç: {args.query}")
    if args.city:
        logger.info(f"üèôÔ∏è –ú—ñ—Å—Ç–æ: {args.city}")
    logger.info(f"üîó URL: {search_url}")
    logger.info(f"üìä –ú–∞–∫—Å–∏–º—É–º –æ–≥–æ–ª–æ—à–µ–Ω—å: {args.max_items}")
    logger.info(f"üîÑ –ú–∞–∫—Å. –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫: {args.concurrent}")
    logger.info(f"üìÅ –õ–æ–≥-—Ñ–∞–π–ª: {log_file}")

    if args.headless:
        logger.info("üñ•Ô∏è –†–µ–∂–∏–º: –±–µ–∑ –≤—ñ–∫–Ω–∞ –±—Ä–∞—É–∑–µ—Ä–∞ (headless)")
    else:
        logger.info("üñ•Ô∏è –†–µ–∂–∏–º: –∑ –≤—ñ–∫–Ω–æ–º –±—Ä–∞—É–∑–µ—Ä–∞")

    if args.no_proxy:
        logger.warning("‚ö†Ô∏è –ü—Ä–æ–∫—Å—ñ –í–ò–ú–ö–ù–ï–ù–û! –¶–µ –Ω–µ–±–µ–∑–ø–µ—á–Ω–æ –¥–ª—è –≤–∞—à–æ–≥–æ IP!")
    else:
        logger.info(f"üîå –ü—Ä–æ–∫—Å—ñ: {len(VALID_PROXY_LIST)} —à—Ç.")

    logger.info("=" * 60)

    # 4. –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø—Ä–æ–∫—Å—ñ
    if not args.no_proxy and not VALID_PROXY_LIST:
        logger.error("‚ùå –ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –ø—Ä–æ–∫—Å—ñ! –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ --no-proxy –∞–±–æ –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è .env —Ñ–∞–π–ª—É")
        return

    # 5. –°—Ç–≤–æ—Ä—é—î–º–æ —Å—Ç–µ–ª—Å –¥–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ —Å–∞–π—Ç—É
    try:
        stealth = get_stealth_for_site('ukraine')
        logger.success("üïµÔ∏è –°—Ç–µ–ª—Å –¥–ª—è OLX —Å—Ç–≤–æ—Ä–µ–Ω–æ")
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç–µ–ª—Å—É: {e}")
        return

    # 6. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ —Å–∫—Ä–∞–ø–µ—Ä
    scraper = Scraper(
        max_items=args.max_items,
        stealth=stealth,
        site_name="OLX",
        max_concurrent=args.concurrent
    )

    # 7. –ó–∞–ø—É—Å–∫–∞—î–º–æ —Å–∫—Ä–∞–ø–µ—Ä
    try:
        logger.info(f"üîÑ –ü–æ—á–∞—Ç–æ–∫ –∑–±–æ—Ä—É –¥–∞–Ω–∏—Ö –∑ OLX...")
        start_time = datetime.now()

        results = await scraper.run(search_url)

        elapsed = datetime.now() - start_time

        # 8. –û–±—Ä–æ–±–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        if results:
            logger.success("=" * 60)
            logger.success("üèÅ –°–ö–†–ê–ü–ï–† –£–°–ü–Ü–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            logger.info(f"üìä –í—Å—å–æ–≥–æ –∑—ñ–±—Ä–∞–Ω–æ: {len(results)} –æ–≥–æ–ª–æ—à–µ–Ω—å")

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü—ñ–Ω–∞—Ö
            total_sum = sum(item.price_value for item in results)
            avg_price = total_sum / len(results) if results else 0
            min_price = min(item.price_value for item in results)
            max_price = max(item.price_value for item in results)

            logger.info(f"üí∞ –°–µ—Ä–µ–¥–Ω—è —Ü—ñ–Ω–∞: {avg_price:,.0f} –≥—Ä–Ω")
            logger.info(f"üíµ –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Ü—ñ–Ω–∞: {min_price:,.0f} –≥—Ä–Ω")
            logger.info(f"üíé –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Ü—ñ–Ω–∞: {max_price:,.0f} –≥—Ä–Ω")
            logger.info(f"‚è±Ô∏è –ß–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {elapsed.total_seconds():.1f} —Å–µ–∫")

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            query_slug = args.query.replace(' ', '_')
            city_slug = f"_{args.city}" if args.city else ""
            base_filename = f"olx_{query_slug}{city_slug}_{timestamp}"

            saved_files = []

            if args.format in ['csv', 'both']:
                csv_file = Exporter.to_csv(results, f"{base_filename}.csv")
                saved_files.append(csv_file)

            if args.format in ['json', 'both']:
                json_file = Exporter.to_json(results, f"{base_filename}.json")
                saved_files.append(json_file)

            logger.info(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ data/:")
            for file in saved_files:
                logger.info(f"   - {file}")
            logger.success("=" * 60)

            # –ü–æ–∫–∞–∑—É—î–º–æ –ø–µ—Ä—à—ñ 5 –æ–≥–æ–ª–æ—à–µ–Ω—å –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É
            logger.info("üìã –ü—Ä–∏–∫–ª–∞–¥–∏ –∑—ñ–±—Ä–∞–Ω–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å:")
            for i, item in enumerate(results[:5], 1):
                # –°–∫–æ—Ä–æ—á—É—î–º–æ –Ω–∞–∑–≤—É –¥–æ 50 —Å–∏–º–≤–æ–ª—ñ–≤
                short_title = item.title[:50] + "..." if len(item.title) > 50 else item.title
                logger.info(f"   {i}. {short_title} | {item.price} | {item.location}")

            # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            today_count = sum(1 for item in results if item.is_today)
            yesterday_count = sum(1 for item in results if item.is_yesterday)

            if today_count > 0 or yesterday_count > 0:
                logger.info(f"üìä –°–≤—ñ–∂—ñ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è:")
                if today_count > 0:
                    logger.info(f"   ‚Ä¢ –°—å–æ–≥–æ–¥–Ω—ñ: {today_count}")
                if yesterday_count > 0:
                    logger.info(f"   ‚Ä¢ –í—á–æ—Ä–∞: {yesterday_count}")

            return results
        else:
            logger.warning("ü§î –û–≥–æ–ª–æ—à–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:")
            logger.warning("   ‚Ä¢ –ó–º—ñ–Ω–∏–ª–∏—Å—å —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –Ω–∞ —Å–∞–π—Ç—ñ")
            logger.warning("   ‚Ä¢ –ü—Ä–æ–±–ª–µ–º–∏ –∑ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è–º")
            logger.warning("   ‚Ä¢ –ë–ª–æ–∫—É–≤–∞–Ω–Ω—è –≤—ñ–¥ OLX")
            logger.warning("   ‚Ä¢ –ù–µ–º–∞—î –æ–≥–æ–ª–æ—à–µ–Ω—å –∑–∞ –∑–∞–ø–∏—Ç–æ–º '{args.query}'")
            return None

    except KeyboardInterrupt:
        logger.warning("\n‚èπÔ∏è –í–∏–∫–æ–Ω–∞–Ω–Ω—è –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
        logger.info("üí° –ü—Ä–æ–≥—Ä–µ—Å –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ checkpoint.json")
        return None
    except Exception as e:
        logger.critical(f"üí• –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        if args.debug:
            import traceback
            logger.error(traceback.format_exc())
        logger.info("üÜò –Ø–∫—â–æ –ø–æ–º–∏–ª–∫–∞ –ø–æ–≤—Ç–æ—Ä—é—î—Ç—å—Å—è, –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ:")
        logger.info("   ‚Ä¢ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –≤ .env")
        logger.info("   ‚Ä¢ –î–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å —Å–∞–π—Ç—É olx.ua")
        logger.info("   ‚Ä¢ –°–µ–ª–µ–∫—Ç–æ—Ä–∏ –≤ –ø–∞—Ä—Å–µ—Ä—ñ")
        return None


def quick_test():
    """–®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å"""
    logger.info("=" * 50)
    logger.info("üîß –®–í–ò–î–ö–ò–ô –¢–ï–°–¢ –ù–ê–õ–ê–®–¢–£–í–ê–ù–¨")
    logger.info("=" * 50)

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —ñ–º–ø–æ—Ä—Ç–∏
    try:
        from src.settings import VALID_PROXY_LIST
        from src.models import OLXItem
        from src.parser import OLXParser

        logger.success("‚úÖ –í—Å—ñ –º–æ–¥—É–ª—ñ —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
        logger.info(f"üìä –ü—Ä–æ–∫—Å—ñ: {len(VALID_PROXY_LIST)} —à—Ç.")

        if len(VALID_PROXY_LIST) == 0:
            logger.warning("‚ö†Ô∏è –ù–µ–º–∞—î –ø—Ä–æ–∫—Å—ñ! –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ .env —Ñ–∞–π–ª")

        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        test_item = OLXItem(
            title="–¢–µ—Å—Ç–æ–≤–µ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è",
            price="1 000 –≥—Ä–Ω.",
            location="–ö–∏—ó–≤",
            date="–°—å–æ–≥–æ–¥–Ω—ñ –æ 12:00",
            url="https://www.olx.ua/test"
        )
        logger.success(f"‚úÖ –ú–æ–¥–µ–ª—å OLXItem –ø—Ä–∞—Ü—é—î: {test_item.price_value} –≥—Ä–Ω")

    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö: {e}")

    logger.info("=" * 50)
    logger.info("‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–æ")


def print_help():
    """–í–∏–≤–æ–¥–∏—Ç—å –¥–æ–≤—ñ–¥–∫—É –ø–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é"""
    help_text = """
    üî∑ –°–ö–†–ê–ü–ï–† –î–õ–Ø OLX.UA üî∑

    –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
      python main.py [–æ–ø—Ü—ñ—ó]

    –ü—Ä–∏–∫–ª–∞–¥–∏:
      python main.py                                      # –ù–æ—É—Ç–±—É–∫–∏ –ø–æ –≤—Å—ñ–π –£–∫—Ä–∞—ó–Ω—ñ
      python main.py --query –Ω–æ—É—Ç–±—É–∫ --max-items 100     # 100 –Ω–æ—É—Ç–±—É–∫—ñ–≤
      python main.py --query —Å–º–∞—Ä—Ç—Ñ–æ–Ω --city –∫–∏—ó–≤        # –°–º–∞—Ä—Ç—Ñ–æ–Ω–∏ –≤ –ö–∏—î–≤—ñ
      python main.py --headless --format json            # –ë–µ–∑ –≤—ñ–∫–Ω–∞, JSON —Ñ–æ—Ä–º–∞—Ç
      python main.py --concurrent 2 --delay 5            # –ü–æ–≤—ñ–ª—å–Ω—ñ—à–µ, –∞–ª–µ –±–µ–∑–ø–µ—á–Ω—ñ—à–µ
      python main.py --debug                              # –î–µ—Ç–∞–ª—å–Ω—ñ –ª–æ–≥–∏

    –û–ø—Ü—ñ—ó:
      --max-items N         –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–≥–æ–ª–æ—à–µ–Ω—å (–∑–∞–∑–≤–∏—á–∞–π 20-50 –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫—É)
      --concurrent N        –ú–∞–∫—Å. –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ (1-3, –±—ñ–ª—å—à–µ = —Ä–∏–∑–∏–∫ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è)
      --headless           –ó–∞–ø—É—Å–∫ –±–µ–∑ –≤—ñ–∫–Ω–∞ –±—Ä–∞—É–∑–µ—Ä–∞ (—à–≤–∏–¥—à–µ, –∞–ª–µ —Å–∫–ª–∞–¥–Ω—ñ—à–µ –¥–µ–±–∞–∂–∏—Ç–∏)
      --no-proxy           –í–∏–º–∫–Ω—É—Ç–∏ –ø—Ä–æ–∫—Å—ñ (–ù–ï–ë–ï–ó–ü–ï–ß–ù–û –¥–ª—è –≤–∞—à–æ–≥–æ IP!)
      --format FORMAT      –§–æ—Ä–º–∞—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è (csv, json, both)
      --delay N           –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
      --query QUERY       –ü–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç (–Ω–æ—É—Ç–±—É–∫, —Å–º–∞—Ä—Ç—Ñ–æ–Ω, –∞–≤—Ç–æ, –∫–≤–∞—Ä—Ç–∏—Ä–∞...)
      --city CITY         –ú—ñ—Å—Ç–æ –¥–ª—è –ø–æ—à—É–∫—É (–∫–∏—ó–≤, –ª—å–≤—ñ–≤, —Ö–∞—Ä–∫—ñ–≤, –æ–¥–µ—Å–∞...)
      --debug             –†–µ–∂–∏–º –Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è (–¥–µ—Ç–∞–ª—å–Ω—ñ –ª–æ–≥–∏)
      --help              –¶—è –¥–æ–≤—ñ–¥–∫–∞
    """
    print(help_text)


if __name__ == "__main__":
    # –Ø–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω–æ –∞—Ä–≥—É–º–µ–Ω—Ç --help, –ø–æ–∫–∞–∑—É—î–º–æ –¥–æ–≤—ñ–¥–∫—É
    if '--help' in sys.argv or '-h' in sys.argv:
        print_help()
        sys.exit(0)

    # –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç (—Ä–æ–∑–∫–æ–º–µ–Ω—Ç—É–π —è–∫—â–æ —Ç—Ä–µ–±–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è)
    # quick_test()

    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ—ó –ø—Ä–æ–≥—Ä–∞–º–∏
    asyncio.run(main())

================================================================================
üìÑ –§–ê–ô–õ: README.md
================================================================================

# Professional Async Web Scraper (E-commerce)

A high-performance, modular web scraping system built with **Python** and **Playwright**. Designed with a focus on data integrity, resilience, and ease of configuration.

## üöÄ Key Features
- **Async Engine:** Powered by `asyncio` and `Playwright` for fast, non-blocking data extraction.
- **Resilience:** Built-in **Checkpoint System** (JSON-based) to resume progress from the last page after any interruption or network failure.
- **Data Validation:** Uses **Pydantic** models to ensure 100% accurate data types (prices, ratings, stocks).
- **Flexible Configuration:** All selectors and URLs are managed via a `config.yaml` file‚Äîno need to touch the code to change targets.
- **Professional Logging:** Detailed execution logs using `loguru`.

## üõ† Tech Stack
- **Language:** Python 3.12+
- **Browser Automation:** Playwright (Chromium)
- **Data Handling:** Pydantic, CSV, YAML
- **Logging:** Loguru

## üìÅ Project Structure
```text
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ models.py       # Data structures & validation
‚îÇ   ‚îú‚îÄ‚îÄ parser.py       # HTML parsing logic
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py      # Core scraping engine & state management
‚îÇ   ‚îî‚îÄ‚îÄ settings.py     # Configuration loader
‚îú‚îÄ‚îÄ config.yaml         # Active configuration (Scraper settings & CSS selectors)
‚îú‚îÄ‚îÄ main.py             # Entry point
‚îî‚îÄ‚îÄ data/               # Output directory (CSV & Checkpoints)
```

##Installation & Usage
Clone the repository:
```bash
git clone [https://github.com/your-username/your-repo-name.git](https://github.com/your-username/your-repo-name.git)
cd your-repo-name
```

## Set up virtual environment & install dependencies:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
playwright install chromium
```

## Configure the scraper: Edit config.yaml to set your start_url and CSS selectors.

Run the scraper:
```bash
python main.py
```

## üìä Output Example
The scraper generates a structured CSV file in the data/ folder: | title | price | rating | availability | url | | :--- | :--- | :--- | :--- | :--- | | A Light in the Attic | ¬£51.77 | Three | In stock | https://... |

## Author: Yaroslav Pauk 

================================================================================
üìÑ –§–ê–ô–õ: requirements.txt
================================================================================

# –Ø–¥—Ä–æ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –±—Ä–∞—É–∑–µ—Ä–æ–º
playwright==1.40.0

# –í–∞–ª—ñ–¥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö —Ç–∞ –º–æ–¥–µ–ª—ñ
pydantic==2.4.1

# –ü—Ä–æ—Ñ–µ—Å—ñ–π–Ω–µ –ª–æ–≥—É–≤–∞–Ω–Ω—è (–∫–æ–ª—å–æ—Ä–æ–≤—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ –∫–æ–Ω—Å–æ–ª—ñ)
loguru==0.7.2

# –î–æ–¥–∞—Ç–∫–æ–≤–∞ –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ Excel (—è–∫—â–æ –∑–∞—Ö–æ—á–µ—à –∑–±–µ—Ä—ñ–≥–∞—Ç–∏ –≤ .xlsx)
#pandas==2.1.3
openpyxl==3.1.2

fake-useragent
pyyaml
requests

#pip install -r requirements.txt
#playwright install chromium

================================================================================
üìÑ –§–ê–ô–õ: src\__init__.py
================================================================================



================================================================================
üìÑ –§–ê–ô–õ: src\client.py
================================================================================

# src/client.py
import os
import random
from playwright.async_api import async_playwright
# –í–∏–¥–∞–ª–µ–Ω–æ AUTH_FILE –∑ —ñ–º–ø–æ—Ä—Ç—É
from src.settings import HEADLESS, USER_AGENTS, TIMEOUT, VALID_PROXY_LIST
from loguru import logger
from fake_useragent import UserAgent


class BrowserClient:
    def __init__(self, proxy: dict = None):
        self.playwright = None
        self.browser = None

        # ===== –ñ–û–†–°–¢–ö–ê –ü–ï–†–ï–í–Ü–†–ö–ê –ü–†–û–ö–°–Ü =====
        if proxy is None:
            if VALID_PROXY_LIST:
                self.proxy = VALID_PROXY_LIST[0]
                logger.info(f"üîå –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤–∏–±—Ä–∞–Ω–æ –ø—Ä–æ–∫—Å—ñ: {self.proxy['server']}")
            else:
                logger.critical("=" * 60)
                logger.critical("üî¥ –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: –°–ü–†–û–ë–ê –°–¢–í–û–†–ï–ù–ù–Ø –ö–õ–Ü–Ñ–ù–¢–ê –ë–ï–ó –ü–†–û–ö–°–Ü!")
                logger.critical("=" * 60)
                logger.critical("üõ°Ô∏è –ó–ê–•–ò–°–¢: –ü—Ä–æ–≥—Ä–∞–º–∞ –±—É–¥–µ –∑—É–ø–∏–Ω–µ–Ω–∞")
                logger.critical("üí° –†—ñ—à–µ–Ω–Ω—è: –î–æ–¥–∞–π—Ç–µ –ø—Ä–æ–∫—Å—ñ –≤ config.yaml –∞–±–æ –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ .env —Ñ–∞–π–ª")
                logger.critical("=" * 60)
                raise Exception("BrowserClient –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ —Å—Ç–≤–æ—Ä–µ–Ω–∏–π –±–µ–∑ –ø—Ä–æ–∫—Å—ñ")
        else:
            self.proxy = proxy

        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö User-Agents
        try:
            self.ua_generator = UserAgent()
            logger.success("‚úÖ fake-useragent —É—Å–ø—ñ—à–Ω–æ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ fake-useragent: {e}. –ë—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ —Ä—É—á–Ω–∏–π —Å–ø–∏—Å–æ–∫.")
            self.ua_generator = None

        # –õ–æ–≥—É—î–º–æ —Å—Ç–∞—Ç—É—Å –ø—Ä–æ–∫—Å—ñ –ø—Ä–∏ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
        if self.proxy:
            masked_proxy = self._mask_proxy_data(self.proxy)
            logger.info(f"üîå –ö–ª—ñ—î–Ω—Ç —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∑ –ø—Ä–æ–∫—Å—ñ: {masked_proxy}")

            proxy_server = self.proxy.get('server', '')
            if 'http://' in proxy_server:
                logger.debug("üì° –¢–∏–ø –ø—Ä–æ–∫—Å—ñ: HTTP")
            elif 'https://' in proxy_server:
                logger.debug("üì° –¢–∏–ø –ø—Ä–æ–∫—Å—ñ: HTTPS")
            elif 'socks' in proxy_server.lower():
                logger.debug("üì° –¢–∏–ø –ø—Ä–æ–∫—Å—ñ: SOCKS")

            if 'username' in self.proxy and 'password' in self.proxy:
                logger.debug("üîê –ü—Ä–æ–∫—Å—ñ –≤–∏–º–∞–≥–∞—î –∞–≤—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—é (–ª–æ–≥—ñ–Ω/–ø–∞—Ä–æ–ª—å)")
        else:
            logger.critical("üî¥ –ö–†–ò–¢–ò–ß–ù–û: –ö–õ–Ü–Ñ–ù–¢ –°–¢–í–û–†–ï–ù–û –ë–ï–ó –ü–†–û–ö–°–Ü!")
            raise Exception("–ö–ª—ñ—î–Ω—Ç –Ω–µ –º–æ–∂–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –±–µ–∑ –ø—Ä–æ–∫—Å—ñ")

    def _mask_proxy_data(self, proxy: dict) -> dict:
        """–ú–∞—Å–∫—É—î —á—É—Ç–ª–∏–≤—ñ –¥–∞–Ω—ñ –ø—Ä–æ–∫—Å—ñ –¥–ª—è –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
        masked = proxy.copy()
        if 'username' in masked:
            username = masked['username']
            masked['username'] = f"{username[:3]}***" if len(username) > 3 else "***"
        if 'password' in masked:
            masked['password'] = '********'
        return masked

    def get_random_ua(self) -> str:
        """–ú–µ—Ç–æ–¥ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –Ω–∞–¥—ñ–π–Ω–æ–≥–æ User-Agent"""
        if self.ua_generator:
            try:
                ua = self.ua_generator.random
                ua_short = ua[:50] + "..." if len(ua) > 50 else ua
                logger.info(f"üåê –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –¥–∏–Ω–∞–º—ñ—á–Ω–∏–π User-Agent: {ua_short}")
                return ua
            except Exception as e:
                logger.warning(f"üì° –ó–±—ñ–π –º–µ—Ä–µ–∂–µ–≤–æ—ó –±–∞–∑–∏ User-Agents: {e}")

        fallback_ua = random.choice(USER_AGENTS)
        logger.info(f"üíæ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ User-Agent –∑ —Ä—É—á–Ω–æ–≥–æ —Å–ø–∏—Å–∫—É: {fallback_ua[:50]}...")
        return fallback_ua

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ –∑ –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–º –ø—Ä–æ–∫—Å—ñ"""
        self.playwright = await async_playwright().start()
        logger.debug("üé≠ Playwright –∑–∞–ø—É—â–µ–Ω–æ")

        # ===== –§–Ü–ù–ê–õ–¨–ù–ê –ü–ï–†–ï–í–Ü–†–ö–ê –ü–ï–†–ï–î –ó–ê–ü–£–°–ö–û–ú =====
        if not self.proxy:
            logger.critical("üî¥ –°–ü–†–û–ë–ê –ó–ê–ü–£–°–ö–£ –ë–†–ê–£–ó–ï–†–ê –ë–ï–ó –ü–†–û–ö–°–Ü!")
            logger.critical("üõ°Ô∏è –ó–ê–•–ò–°–¢: –ó–∞–ø—É—Å–∫ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–æ")
            logger.debug(f"VALID_PROXY_LIST: {VALID_PROXY_LIST}")
            logger.debug(f"self.proxy: {self.proxy}")
            raise Exception("–ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ –±–µ–∑ –ø—Ä–æ–∫—Å—ñ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–æ!")

        launch_options = {
            "headless": HEADLESS,
            "proxy": self._mask_proxy_data(self.proxy) if self.proxy else None
        }
        logger.info(f"üîå –ó–ê–ü–£–°–ö –ó –ü–†–û–ö–°–Ü: {self._mask_proxy_data(self.proxy)['server']}")
        logger.debug(f"‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞–ø—É—Å–∫—É –±—Ä–∞—É–∑–µ—Ä–∞: {launch_options}")

        try:
            self.browser = await self.playwright.chromium.launch(
                headless=HEADLESS,
                proxy=self.proxy if self.proxy else None
            )

            browser_version = self.browser.version
            logger.info(f"üöÄ –ë—Ä–∞—É–∑–µ—Ä Chromium v{browser_version} –∑–∞–ø—É—â–µ–Ω–æ")
            logger.success(f"üîå –ü—Ä–æ–∫—Å—ñ –ø—ñ–¥–∫–ª—é—á–µ–Ω–æ: {self._mask_proxy_data(self.proxy)['server']}")

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–ø—É—Å–∫—É –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            if self.proxy:
                logger.error(f"üî¥ –ú–æ–∂–ª–∏–≤–∞ –ø—Ä–æ–±–ª–µ–º–∞ –∑ –ø—Ä–æ–∫—Å—ñ: {self._mask_proxy_data(self.proxy)['server']}")
                logger.error("üí° –ü–µ—Ä–µ–≤—ñ—Ä: 1) –ß–∏ –ø—Ä–∞—Ü—é—î –ø—Ä–æ–∫—Å—ñ? 2) –ß–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç?")
            raise e

    async def stop(self):
        """–ü–æ–≤–Ω–µ –∑–∞–∫—Ä–∏—Ç—Ç—è –±—Ä–∞—É–∑–µ—Ä–∞"""
        try:
            if self.browser:
                await self.browser.close()
                logger.debug("üõë –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä–∏—Ç–æ")
            if self.playwright:
                await self.playwright.stop()
                logger.debug("üé≠ Playwright –∑—É–ø–∏–Ω–µ–Ω–æ")
            logger.success("üõë –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π –∫–ª—ñ—î–Ω—Ç –ø–æ–≤–Ω—ñ—Å—Ç—é –∑—É–ø–∏–Ω–µ–Ω–æ.")
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑—É–ø–∏–Ω—Ü—ñ –∫–ª—ñ—î–Ω—Ç–∞: {e}")

    async def check_proxy_health(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –ø—Ä–∞—Ü–µ–∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –ø—Ä–æ–∫—Å—ñ —á–µ—Ä–µ–∑ httpbin"""
        if not self.proxy:
            logger.critical("üî¥ –ù–ï–ú–ê–Ñ –ü–†–û–ö–°–Ü –î–õ–Ø –ü–ï–†–ï–í–Ü–†–ö–ò!")
            return False

        try:
            context = await self.browser.new_context()
            page = await context.new_page()

            logger.info(f"ü©∫ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–∞—Ü–µ–∑–¥–∞—Ç–Ω–æ—Å—Ç—ñ –ø—Ä–æ–∫—Å—ñ: {self._mask_proxy_data(self.proxy)['server']}")
            await page.goto("https://httpbin.org/ip", timeout=10000)
            content = await page.text_content("body")

            await context.close()
            logger.success(f"‚úÖ –ü—Ä–æ–∫—Å—ñ –ø—Ä–∞—Ü—é—î! –í—ñ–¥–ø–æ–≤—ñ–¥—å: {content}")
            return True

        except Exception as e:
            logger.error(f"‚ùå –ü—Ä–æ–∫—Å—ñ –Ω–µ –ø—Ä–∞—Ü—é—î: {e}")
            return False

================================================================================
üìÑ –§–ê–ô–õ: src\exporter.py
================================================================================

import csv
import json
from typing import Any
from src.settings import DATA_DIR
from loguru import logger


class Exporter:
    @staticmethod
    def append_to_csv(item: Any, filename: str = "live_results.csv"):
        """Append a single record to a CSV file (works with any Pydantic model)"""
        filepath = DATA_DIR / filename
        file_exists = filepath.exists()

        try:
            # Use model_dump() to convert Pydantic object to a dictionary
            row = item.model_dump()
            fieldnames = row.keys()

            with open(filepath, 'a', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                if not file_exists:
                    writer.writeheader()
                writer.writerow(row)
        except Exception as e:
            logger.error(f"‚ùå Error while appending to CSV: {e}")

    @staticmethod
    def to_csv(data: list[Any], filename: str = "results.csv"):
        """Save the entire list of objects to a CSV file"""
        if not data:
            logger.warning("‚ö†Ô∏è No data available for CSV export.")
            return

        filepath = DATA_DIR / filename
        try:
            fieldnames = data[0].model_dump().keys()
            with open(filepath, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for item in data:
                    writer.writerow(item.model_dump())
            logger.success(f"üíæ All data successfully saved to CSV: {filepath}")
        except Exception as e:
            logger.error(f"‚ùå Error during full CSV export: {e}")

    @staticmethod
    def to_json(data: list[Any], filename: str = "results.json"):
        """Save data to a JSON file"""
        if not data:
            return

        filepath = DATA_DIR / filename
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json_data = [item.model_dump() for item in data]
                json.dump(json_data, f, ensure_ascii=False, indent=4)
            logger.success(f"üíæ Data successfully saved to JSON: {filepath}")
        except Exception as e:
            logger.error(f"‚ùå Error during JSON export: {e}")

================================================================================
üìÑ –§–ê–ô–õ: src\models.py
================================================================================

# src/models.py
from pydantic import BaseModel, Field, field_validator
from typing import Optional, List
import re


class OLXItem(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –∑ OLX"""

    # –û—Å–Ω–æ–≤–Ω—ñ –ø–æ–ª—è
    title: str = Field(..., description="–ù–∞–∑–≤–∞ —Ç–æ–≤–∞—Ä—É", min_length=1)
    price: str = Field(..., description="–¶—ñ–Ω–∞ —è–∫ —Ç–µ–∫—Å—Ç (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ '9 000 –≥—Ä–Ω.')")
    location: str = Field(..., description="–ú—ñ—Å—Ç–æ/–æ–±–ª–∞—Å—Ç—å")
    date: str = Field(..., description="–î–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó")
    url: str = Field(..., description="–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è")

    # –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ –ø–æ–ª—è
    image_url: Optional[str] = Field(None, description="–ü–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è")

    # –í–∞–ª—ñ–¥–∞—Ç–æ—Ä–∏
    @field_validator('price', mode='before')
    @classmethod
    def clean_price(cls, v: str) -> str:
        """–û—á–∏—â–∞—î —Ü—ñ–Ω—É –≤—ñ–¥ –∑–∞–π–≤–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤"""
        if isinstance(v, str):
            return v.strip()
        return str(v)

    @field_validator('title', 'location', 'date', mode='before')
    @classmethod
    def clean_text(cls, v: str) -> str:
        """–û—á–∏—â–∞—î —Ç–µ–∫—Å—Ç –≤—ñ–¥ –∑–∞–π–≤–∏—Ö –ø—Ä–æ–±—ñ–ª—ñ–≤"""
        if isinstance(v, str):
            return ' '.join(v.split())  # –≤–∏–¥–∞–ª—è—î –∑–∞–π–≤—ñ –ø—Ä–æ–±—ñ–ª–∏
        return str(v)

    @field_validator('url', mode='before')
    @classmethod
    def validate_url(cls, v: str) -> str:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —â–æ URL –ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è –∑ https://"""
        if isinstance(v, str):
            if v.startswith('//'):
                return f"https:{v}"
            if not v.startswith('http'):
                return f"https://www.olx.ua{v}"
        return v

    # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –≤–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ –¥–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ
    @property
    def price_value(self) -> int:
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î —á–∏—Å–ª–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è —Ü—ñ–Ω–∏
        –ù–∞–ø—Ä–∏–∫–ª–∞–¥: '9 000 –≥—Ä–Ω.' -> 9000
        """
        if not self.price:
            return 0
        # –í–∏–¥–∞–ª—è—î –≤—Å—ñ –Ω–µ—Ü–∏—Ñ—Ä–æ–≤—ñ —Å–∏–º–≤–æ–ª–∏
        digits = re.sub(r'[^\d]', '', self.price)
        return int(digits) if digits else 0

    @property
    def city(self) -> str:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Ç—ñ–ª—å–∫–∏ –Ω–∞–∑–≤—É –º—ñ—Å—Ç–∞ (–±–µ–∑ –æ–±–ª–∞—Å—Ç—ñ)"""
        # –Ø–∫—â–æ –ª–æ–∫–∞—Ü—ñ—è –º–∞—î —Ñ–æ—Ä–º–∞—Ç "–ö–∏—ó–≤, –ì–æ–ª–æ—Å—ñ—ó–≤—Å—å–∫–∏–π"
        if ',' in self.location:
            return self.location.split(',')[0].strip()
        return self.location

    @property
    def is_today(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–µ —Å—å–æ–≥–æ–¥–Ω—ñ"""
        return '–°—å–æ–≥–æ–¥–Ω—ñ' in self.date

    @property
    def is_yesterday(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –æ–ø—É–±–ª—ñ–∫–æ–≤–∞–Ω–µ –≤—á–æ—Ä–∞"""
        return '–í—á–æ—Ä–∞' in self.date

    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î –≤ —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è"""
        return {
            'title': self.title,
            'price': self.price,
            'price_value': self.price_value,
            'location': self.location,
            'city': self.city,
            'date': self.date,
            'is_today': self.is_today,
            'url': self.url,
            'image_url': self.image_url
        }

    def __str__(self) -> str:
        """–ö–æ—Ä–æ—Ç–∫–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è"""
        return f"{self.title[:50]}... | {self.price} | {self.location}"


class ScraperResult(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É"""
    total_found: int = Field(..., description="–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å")
    items: List[OLXItem] = Field(default_factory=list, description="–°–ø–∏—Å–æ–∫ –æ–≥–æ–ª–æ—à–µ–Ω—å")

    @property
    def total_price(self) -> int:
        """–°—É–º–∞—Ä–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –≤—Å—ñ—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å"""
        return sum(item.price_value for item in items)

    @property
    def average_price(self) -> float:
        """–°–µ—Ä–µ–¥–Ω—è —Ü—ñ–Ω–∞"""
        if not self.items:
            return 0
        return self.total_price / len(self.items)

    def to_dict(self) -> dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î –≤ —Å–ª–æ–≤–Ω–∏–∫ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è"""
        return {
            'total_found': self.total_found,
            'items': [item.to_dict() for item in self.items],
            'statistics': {
                'total_price': self.total_price,
                'average_price': round(self.average_price, 2),
                'items_count': len(self.items)
            }
        }

================================================================================
üìÑ –§–ê–ô–õ: src\parser.py
================================================================================

# src/parser.py
from playwright.async_api import Page
from src.models import OLXItem
from loguru import logger
from urllib.parse import urljoin
import asyncio


class OLXParser:
    """Parser for olx.ua listings"""

    def __init__(self):
        # –†–æ–∑—à–∏—Ä–µ–Ω—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –∑ –∫—ñ–ª—å–∫–æ–º–∞ –≤–∞—Ä—ñ–∞–Ω—Ç–∞–º–∏
        self.selectors = {
            "card": [
                "div.css-1g5933j",  # –æ—Å–Ω–æ–≤–Ω–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è 1-—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                "div.css-1ap2g7h",  # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –¥–ª—è –Ω–∞—Å—Ç—É–ø–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫
                "div[data-cy='l-card']",  # data-cy –∞—Ç—Ä–∏–±—É—Ç
                "div.css-1venxjw",  # —â–µ –æ–¥–∏–Ω –≤–∞—Ä—ñ–∞–Ω—Ç
                "div.css-1ww9k7w"  # –∑–∞–ø–∞—Å–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
            ],
            "title": [
                "h4.css-hzlye5",
                "h4.css-1juy1o3",  # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –∫–ª–∞—Å
                "a.css-z3gu2d h4"  # –≤–∫–ª–∞–¥–µ–Ω–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä
            ],
            "price": [
                "p[data-testid='ad-price']",
                "p.css-10b0gli",
                "h3.css-1ap2g7h"
            ],
            "location_date": [
                "p[data-testid='location-date']",
                "p.css-1mwad3y",
                "span.css-1c0ed4l"
            ],
            "link": [
                "a.css-1tqlkj0",
                "a[data-testid='ad-card-link']",
                "a.css-z3gu2d"
            ],
            "image": [
                "img.css-8wsg1m",
                "img[data-testid='ad-image']",
                "img.css-8wsg5m"
            ],
            "next_button": [
                "a[data-testid='pagination-forward']",
                "a[rel='next']",
                "li.next a",
                "a.css-1sxq6i0"
            ]
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –¥–µ–±–∞–≥—É
        self.stats = {
            'pages_processed': 0,
            'cards_found': 0,
            'cards_parsed': 0,
            'errors': 0
        }

    async def _find_working_selector(self, page: Page, selector_list: list, timeout: int = 5000) -> str | None:
        """
        –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –ø–µ—Ä—à–∏–π —Ä–æ–±–æ—á–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä –∑—ñ —Å–ø–∏—Å–∫—É

        Args:
            page: –°—Ç–æ—Ä—ñ–Ω–∫–∞ Playwright
            selector_list: –°–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
            timeout: –ß–∞—Å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –≤ –º—Å

        Returns:
            str: –†–æ–±–æ—á–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä –∞–±–æ None
        """
        for selector in selector_list:
            try:
                # –®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑ –º–∞–ª–∏–º —Ç–∞–π–º–∞—É—Ç–æ–º
                element = await page.wait_for_selector(selector, timeout=timeout, state='attached')
                if element:
                    logger.debug(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ —Ä–æ–±–æ—á–∏–π —Å–µ–ª–µ–∫—Ç–æ—Ä: {selector}")
                    return selector
            except:
                continue
        return None

    async def parse_listings(self, page: Page) -> list[OLXItem]:
        """
        Parse OLX listings from the page with improved error handling
        Extracts: title, price, location, date, image, and URL
        """
        items = []
        self.stats['pages_processed'] += 1

        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –Ω–æ–º–µ—Ä —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        current_url = page.url
        page_num = current_url.split('page=')[-1] if 'page=' in current_url else '1'

        logger.info(f"üìÑ –û–±—Ä–æ–±–∫–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page_num}...")

        try:
            # –ß–µ–∫–∞—î–º–æ –∑ –±—ñ–ª—å—à–∏–º —Ç–∞–π–º–∞—É—Ç–æ–º (30 —Å–µ–∫—É–Ω–¥)
            card_selector = await self._find_working_selector(page, self.selectors["card"], timeout=30000)

            if not card_selector:
                logger.error(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∂–æ–¥–Ω–æ–≥–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∫–∞—Ä—Ç–æ–∫ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page_num}")
                return items

            # –û—Ç—Ä–∏–º—É—î–º–æ –≤—Å—ñ –∫–∞—Ä—Ç–∫–∏
            cards = await page.query_selector_all(card_selector)
            self.stats['cards_found'] += len(cards)

            logger.info(f"üì¶ –ó–Ω–∞–π–¥–µ–Ω–æ {len(cards)} –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page_num}")

            for card_index, card in enumerate(cards, 1):
                try:
                    # Title
                    title_selector = await self._find_working_selector(card, self.selectors["title"], timeout=1000)
                    title_el = await card.query_selector(title_selector) if title_selector else None
                    title = await title_el.inner_text() if title_el else "–ë–µ–∑ –Ω–∞–∑–≤–∏"

                    # Price
                    price_selector = await self._find_working_selector(card, self.selectors["price"], timeout=1000)
                    price_el = await card.query_selector(price_selector) if price_selector else None
                    price_text = await price_el.inner_text() if price_el else "0 –≥—Ä–Ω"

                    # Location and date
                    loc_selector = await self._find_working_selector(card, self.selectors["location_date"],
                                                                     timeout=1000)
                    loc_el = await card.query_selector(loc_selector) if loc_selector else None
                    loc_text = await loc_el.inner_text() if loc_el else ""

                    location = ""
                    date = ""
                    if " - " in loc_text:
                        parts = loc_text.split(" - ")
                        location = parts[0].strip()
                        date = parts[1].strip() if len(parts) > 1 else ""

                    # URL
                    link_selector = await self._find_working_selector(card, self.selectors["link"], timeout=1000)
                    link_el = await card.query_selector(link_selector) if link_selector else None
                    link = await link_el.get_attribute("href") if link_el else ""

                    # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è
                    if link:
                        if link.startswith('//'):
                            full_url = f"https:{link}"
                        elif link.startswith('/'):
                            full_url = urljoin("https://www.olx.ua", link)
                        else:
                            full_url = link
                    else:
                        full_url = ""

                    # Image
                    img_selector = await self._find_working_selector(card, self.selectors["image"], timeout=1000)
                    img_el = await card.query_selector(img_selector) if img_selector else None
                    img_url = await img_el.get_attribute("src") if img_el else ""
                    if img_url and img_url.startswith("//"):
                        img_url = "https:" + img_url

                    # Create OLX item
                    item = OLXItem(
                        title=title.strip(),
                        price=price_text.strip(),
                        location=location,
                        date=date,
                        url=full_url,
                        image_url=img_url if img_url else None
                    )
                    items.append(item)
                    self.stats['cards_parsed'] += 1

                    # –õ–æ–≥—É—î–º–æ –∫–æ–∂–Ω—É 10-—Ç—É –∫–∞—Ä—Ç–∫—É –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—É
                    if card_index % 10 == 0:
                        logger.debug(f"   –ü—Ä–æ–≥—Ä–µ—Å: {card_index}/{len(cards)}")

                except Exception as e:
                    self.stats['errors'] += 1
                    logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –∫–∞—Ä—Ç–∫–∏ #{card_index}: {e}")
                    continue

            logger.success(f"‚úÖ –£—Å–ø—ñ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ {len(items)} –æ–≥–æ–ª–æ—à–µ–Ω—å –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page_num}")

        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ {page_num}: {e}")

            # Debug: –∑–±–µ—Ä—ñ–≥–∞—î–º–æ HTML –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
            if page_num == '1':
                html = await page.content()
                with open("debug_page_1.html", "w", encoding="utf-8") as f:
                    f.write(html)
                logger.info("üíæ HTML –ø–µ—Ä—à–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")

        return items

    async def get_next_page(self, page: Page) -> str | None:
        """
        Get next page URL for OLX with improved selector handling

        Returns:
            str: URL –Ω–∞—Å—Ç—É–ø–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∞–±–æ None
        """
        current_url = page.url
        page_num = current_url.split('page=')[-1] if 'page=' in current_url else '1'

        logger.info(f"üîç –®—É–∫–∞—é –∫–Ω–æ–ø–∫—É '–î–∞–ª—ñ' –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ {page_num}...")

        # –°–ø—Ä–æ–±—É—î–º–æ –≤—Å—ñ —Å–µ–ª–µ–∫—Ç–æ—Ä–∏ –¥–ª—è –∫–Ω–æ–ø–∫–∏
        for selector in self.selectors["next_button"]:
            try:
                next_button = await page.query_selector(selector)
                if next_button:
                    href = await next_button.get_attribute("href")
                    if href:
                        # –Ø–∫—â–æ href - —Ü–µ –ø—Ä–æ—Å—Ç–∏–π –Ω–æ–º–µ—Ä —Å—Ç–æ—Ä—ñ–Ω–∫–∏
                        if href.isdigit():
                            base_url = current_url.split('?')[0]
                            if '?' in current_url:
                                params = current_url.split('?')[1]
                                next_url = f"{base_url}?page={href}"
                            else:
                                next_url = f"{base_url}?page={href}"
                        else:
                            next_url = urljoin(page.url, href)

                        logger.success(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –Ω–∞—Å—Ç—É–ø–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É: {next_url}")
                        return next_url
            except:
                continue

        # –Ø–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ –∫–Ω–æ–ø–∫—É, –ø—Ä–æ–±—É—î–º–æ —Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ URL –≤—Ä—É—á–Ω—É
        if 'page=' in current_url:
            # –ó–±—ñ–ª—å—à—É—î–º–æ –Ω–æ–º–µ—Ä —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–∞ 1
            import re
            match = re.search(r'page=(\d+)', current_url)
            if match:
                current_page = int(match.group(1))
                next_page = current_page + 1
                next_url = re.sub(r'page=\d+', f'page={next_page}', current_url)
                logger.info(f"üîÑ –°–ø—Ä–æ–±—É—é —Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ URL –≤—Ä—É—á–Ω—É: {next_url}")
                return next_url
        elif 'q-' in current_url:
            # –ü–µ—Ä—à–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞, –¥–æ–¥–∞—î–º–æ page=2
            if '?' in current_url:
                next_url = f"{current_url}&page=2"
            else:
                next_url = f"{current_url}?page=2"
            logger.info(f"üîÑ –°–ø—Ä–æ–±—É—é —Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ URL –¥–ª—è –¥—Ä—É–≥–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {next_url}")
            return next_url

        logger.info(f"üèÅ –¶–µ –æ—Å—Ç–∞–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∞ ({page_num})")
        return None

    def print_stats(self):
        """–í–∏–≤–æ–¥–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∏ –ø–∞—Ä—Å–µ—Ä–∞"""
        logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞:")
        logger.info(f"   ‚Ä¢ –°—Ç–æ—Ä—ñ–Ω–æ–∫ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {self.stats['pages_processed']}")
        logger.info(f"   ‚Ä¢ –ö–∞—Ä—Ç–æ–∫ –∑–Ω–∞–π–¥–µ–Ω–æ: {self.stats['cards_found']}")
        logger.info(f"   ‚Ä¢ –ö–∞—Ä—Ç–æ–∫ —Å–ø–∞—Ä—Å–µ–Ω–æ: {self.stats['cards_parsed']}")
        logger.info(f"   ‚Ä¢ –ü–æ–º–∏–ª–æ–∫: {self.stats['errors']}")
        if self.stats['cards_found'] > 0:
            success_rate = (self.stats['cards_parsed'] / self.stats['cards_found']) * 100
            logger.info(f"   ‚Ä¢ –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {success_rate:.1f}%")

================================================================================
üìÑ –§–ê–ô–õ: src\proxy_fetcher.py
================================================================================

# src/proxy_fetcher.py
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –∑ Webshare API
–ü—ñ–¥—Ç—Ä–∏–º—É—î 100+ –ø—Ä–æ–∫—Å—ñ —á–µ—Ä–µ–∑ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—é
"""

import requests
import os
from pathlib import Path
from loguru import logger
import yaml
from dotenv import load_dotenv

load_dotenv()


class WebshareProxyFetcher:
    """–ö–ª–∞—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –∑ Webshare"""

    def __init__(self, api_token: str = None):
        """
        Args:
            api_token: API –∫–ª—é—á –∑ Webshare (—è–∫—â–æ None, –±–µ—Ä–µ –∑ .env)
        """
        self.api_token = api_token or os.getenv("WEBSHARE_API_TOKEN")
        if not self.api_token:
            raise ValueError("‚ùå API token –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ! –î–æ–¥–∞–π—Ç–µ WEBSHARE_API_TOKEN –≤ .env —Ñ–∞–π–ª")

        self.base_url = "https://proxy.webshare.io/api/v2"
        self.headers = {
            "Authorization": f"Token {self.api_token}",
            "Content-Type": "application/json"
        }
        self.proxies = []
        self.username = os.getenv("WEBSHARE_USERNAME")
        self.password = os.getenv("WEBSHARE_PASSWORD")

    def fetch_all_proxies(self) -> list:
        """
        –û—Ç—Ä–∏–º—É—î –í–°–Ü –ø—Ä–æ–∫—Å—ñ —á–µ—Ä–µ–∑ API –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—é –ø–∞–≥—ñ–Ω–∞—Ü—ñ—î—é
        –ü—ñ–¥—Ç—Ä–∏–º—É—î 100, 500, 1000+ –ø—Ä–æ–∫—Å—ñ

        Returns:
            list: –°–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö –ø—Ä–æ–∫—Å—ñ
        """
        all_proxies = []
        page = 1
        page_size = 100  # –ú–∞–∫—Å–∏–º—É–º 100 –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫—É

        logger.info("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –ø—Ä–æ–∫—Å—ñ –∑ Webshare...")

        while True:
            try:
                # –î–æ–¥–∞—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä mode
                url = f"{self.base_url}/proxy/list/?page={page}&page_size={page_size}&mode=direct"

                response = requests.get(
                    url,
                    headers=self.headers,
                    timeout=10
                )

                if response.status_code == 200:
                    data = response.json()
                    results = data.get("results", [])

                    if not results:
                        break

                    for proxy in results:
                        proxy_config = {
                            "server": f"http://{proxy['proxy_address']}:{proxy['port']}",
                            "username": proxy['username'],
                            "password": proxy['password']
                        }
                        all_proxies.append(proxy_config)

                    logger.info(f"üì¶ –°—Ç–æ—Ä—ñ–Ω–∫–∞ {page}: +{len(results)} –ø—Ä–æ–∫—Å—ñ (–≤—Å—å–æ–≥–æ: {len(all_proxies)})")

                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î –Ω–∞—Å—Ç—É–ø–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞
                    if data.get("next"):
                        page += 1
                    else:
                        break

                elif response.status_code == 429:
                    logger.warning("‚è≥ Rate limit, –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è 10 —Å–µ–∫—É–Ω–¥...")
                    import time
                    time.sleep(10)
                    continue
                else:
                    logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ API: {response.status_code} - {response.text}")
                    break

            except Exception as e:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
                break

        self.proxies = all_proxies
        logger.success(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –í–°–Ü –ø—Ä–æ–∫—Å—ñ: {len(all_proxies)} —à—Ç.")

        return all_proxies

    def get_proxy_list(self, mode: str = "direct") -> list:
        """
        –ü—Ä–æ—Å—Ç–∏–π –º–µ—Ç–æ–¥ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –∑ –ø–µ—Ä—à–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏

        Args:
            mode: "direct" –∞–±–æ "backbone"
        """
        try:
            url = f"{self.base_url}/proxy/list/?mode={mode}"
            response = requests.get(url, headers=self.headers, timeout=10)

            if response.status_code == 200:
                data = response.json()
                proxies = []

                for proxy in data.get("results", []):
                    proxy_config = {
                        "server": f"http://{proxy['proxy_address']}:{proxy['port']}",
                        "username": proxy['username'],
                        "password": proxy['password']
                    }
                    proxies.append(proxy_config)

                logger.success(f"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ {len(proxies)} –ø—Ä–æ–∫—Å—ñ")
                return proxies
            else:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {response.status_code} - {response.text}")
                return []

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
            return []

    def get_rotating_endpoint(self) -> dict:
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î –∫–æ–Ω—Ñ—ñ–≥ –¥–ª—è —Ä–æ—Ç–∞—Ü—ñ–π–Ω–æ–≥–æ –µ–Ω–¥–ø–æ—ñ–Ω—Ç—É

        Returns:
            dict: –ö–æ–Ω—Ñ—ñ–≥ –¥–ª—è —Ä–æ—Ç–∞—Ü—ñ–π–Ω–æ–≥–æ –ø—Ä–æ–∫—Å—ñ
        """
        return {
            "server": "http://p.webshare.io:80",
            "username": self.username,
            "password": self.password
        }

    def save_to_yaml(self, proxies: list, config_path: str = "config.yaml"):
        """
        –ó–±–µ—Ä—ñ–≥–∞—î –ø—Ä–æ–∫—Å—ñ –≤ YAML —Ñ–∞–π–ª

        Args:
            proxies: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ
            config_path: –®–ª—è—Ö –¥–æ config.yaml
        """
        try:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î —Ñ–∞–π–ª
            config_file = Path(config_path)

            if config_file.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f) or {}
            else:
                config = {}

            # –û–Ω–æ–≤–ª—é—î–º–æ –ø—Ä–æ–∫—Å—ñ
            config['proxies'] = proxies

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –Ω–∞–∑–∞–¥
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, allow_unicode=True, default_flow_style=False, indent=2)

            logger.success(f"‚úÖ {len(proxies)} –ø—Ä–æ–∫—Å—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {config_path}")

        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {e}")


# –ó—Ä—É—á–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
def update_proxies_from_webshare(use_rotating: bool = False):
    """
    –û–Ω–æ–≤–ª—é—î –ø—Ä–æ–∫—Å—ñ –∑ Webshare

    Args:
        use_rotating: –Ø–∫—â–æ True - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î —Ä–æ—Ç–∞—Ü—ñ–π–Ω–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç,
                      —è–∫—â–æ False - –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –≤—Å—ñ –ø—Ä–æ–∫—Å—ñ
    """
    fetcher = WebshareProxyFetcher()

    if use_rotating:
        # –ü—Ä–æ—Å—Ç–∏–π —Ä–æ—Ç–∞—Ü—ñ–π–Ω–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç (–æ–¥–∏–Ω –∑–∞–ø–∏—Å)
        proxies = [fetcher.get_rotating_endpoint()]
        logger.info("üîÑ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é —Ä–æ—Ç–∞—Ü—ñ–π–Ω–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç")

        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ YAML
        fetcher.save_to_yaml(proxies)
        return proxies
    else:
        # –í—Å—ñ –ø—Ä–æ–∫—Å—ñ –∑ –ø–∞–≥—ñ–Ω–∞—Ü—ñ—î—é (–¥–ª—è 100+ —à—Ç)
        proxies = fetcher.fetch_all_proxies()

        if proxies:
            fetcher.save_to_yaml(proxies)
            return proxies
        else:
            # –Ø–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è, –ø—Ä–æ–±—É—î–º–æ –ø—Ä–æ—Å—Ç–∏–π –º–µ—Ç–æ–¥
            logger.warning("‚ö†Ô∏è –°–ø—Ä–æ–±—É—é –ø—Ä–æ—Å—Ç–∏–π –º–µ—Ç–æ–¥ get_proxy_list...")
            proxies = fetcher.get_proxy_list()
            if proxies:
                fetcher.save_to_yaml(proxies)
                return proxies

            logger.warning("‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø—Ä–æ–∫—Å—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é —Å—Ç–∞—Ä—ñ")
            return None

================================================================================
üìÑ –§–ê–ô–õ: src\proxy_monitor.py
================================================================================

# src/proxy_monitor.py
"""
–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞–Ω—É –ø—Ä–æ–∫—Å—ñ —Ç–∞ –∑–∞—Ö–∏—Å—Ç –≤—ñ–¥ –≤–∏—Ç–æ–∫—É IP
"""

from loguru import logger
from src.settings import VALID_PROXY_LIST
import time
from collections import deque
from datetime import datetime


class ProxyMonitor:
    """–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ"""

    def __init__(self):
        self.usage_stats = {}
        for proxy in VALID_PROXY_LIST:
            server = proxy['server']
            self.usage_stats[server] = {
                'used': 0,
                'failed': 0,
                'last_used': None,
                'total_time': 0,
                'fastest': float('inf'),
                'slowest': 0
            }

        self.recent_failures = deque(maxlen=100)
        self.start_time = time.time()
        self.total_requests = 0
        self.successful_requests = 0

        logger.info(f"üìä ProxyMonitor —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –¥–ª—è {len(VALID_PROXY_LIST)} –ø—Ä–æ–∫—Å—ñ")

    def log_proxy_usage(self, proxy_server: str, success: bool, response_time: float = None):
        """–õ–æ–≥—É—î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ"""
        if proxy_server in self.usage_stats:
            self.usage_stats[proxy_server]['used'] += 1
            self.usage_stats[proxy_server]['last_used'] = datetime.now().strftime('%H:%M:%S')
            self.total_requests += 1

            if response_time:
                self.usage_stats[proxy_server]['total_time'] += response_time
                if response_time < self.usage_stats[proxy_server]['fastest']:
                    self.usage_stats[proxy_server]['fastest'] = response_time
                if response_time > self.usage_stats[proxy_server]['slowest']:
                    self.usage_stats[proxy_server]['slowest'] = response_time

            if not success:
                self.usage_stats[proxy_server]['failed'] += 1
                self.recent_failures.append({
                    'proxy': proxy_server,
                    'time': datetime.now().strftime('%H:%M:%S'),
                    'error': 'Connection failed'
                })
            else:
                self.successful_requests += 1

    def check_proxy_health(self) -> bool:
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ —î —Ö–æ—á –æ–¥–Ω–µ —Ä–æ–±–æ—á–µ –ø—Ä–æ–∫—Å—ñ"""
        if not VALID_PROXY_LIST:
            logger.critical("=" * 60)
            logger.critical("üî¥ –ö–†–ò–¢–ò–ß–ù–û: –í–°–Ü –ü–†–û–ö–°–Ü –ú–ï–†–¢–í–Ü!")
            logger.critical("=" * 60)
            logger.critical("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–º–∏–ª–æ–∫:")
            for failure in list(self.recent_failures)[-5:]:  # –û—Å—Ç–∞–Ω–Ω—ñ 5 –ø–æ–º–∏–ª–æ–∫
                logger.critical(f"   ‚Ä¢ {failure['time']} - {failure['proxy']}")
            logger.critical("=" * 60)
            logger.critical("üõ°Ô∏è –ó–ê–•–ò–°–¢: –ü—Ä–æ–≥—Ä–∞–º–∞ –±—É–¥–µ –∑—É–ø–∏–Ω–µ–Ω–∞")
            logger.critical("üí° –†—ñ—à–µ–Ω–Ω—è: –û–Ω–æ–≤—ñ—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ –≤ config.yaml")
            logger.critical("=" * 60)
            return False
        return True

    def print_stats(self):
        """–í–∏–≤–æ–¥–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ"""
        logger.info("=" * 70)
        logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –í–ò–ö–û–†–ò–°–¢–ê–ù–ù–Ø –ü–†–û–ö–°–Ü")
        logger.info("=" * 70)

        # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        runtime = int(time.time() - self.start_time)
        hours = runtime // 3600
        minutes = (runtime % 3600) // 60
        seconds = runtime % 60

        logger.info(f"‚è±Ô∏è  –ß–∞—Å —Ä–æ–±–æ—Ç–∏: {hours}–≥ {minutes}—Ö–≤ {seconds}—Å")
        logger.info(f"üìà –í—Å—å–æ–≥–æ –∑–∞–ø–∏—Ç—ñ–≤: {self.total_requests}")
        logger.info(f"‚úÖ –£—Å–ø—ñ—à–Ω–∏—Ö: {self.successful_requests}")
        logger.info(f"‚ùå –ü–æ–º–∏–ª–æ–∫: {self.total_requests - self.successful_requests}")

        if self.total_requests > 0:
            success_rate = (self.successful_requests / self.total_requests) * 100
            logger.info(f"üìä –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {success_rate:.1f}%")

        logger.info("-" * 70)
        logger.info("üìã –î–µ—Ç–∞–ª—ñ –ø–æ –∫–æ–∂–Ω–æ–º—É –ø—Ä–æ–∫—Å—ñ:")
        logger.info("-" * 70)

        for proxy, stats in self.usage_stats.items():
            if stats['used'] > 0:
                status = "‚úÖ" if stats['failed'] == 0 else "‚ö†Ô∏è"
                success_rate = ((stats['used'] - stats['failed']) / stats['used']) * 100

                avg_time = stats['total_time'] / stats['used'] if stats['used'] > 0 else 0

                logger.info(f"   {status} {proxy}")
                logger.info(f"      –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ: {stats['used']} —Ä–∞–∑—ñ–≤")
                logger.info(f"      –ü–æ–º–∏–ª–æ–∫: {stats['failed']}")
                logger.info(f"      –£—Å–ø—ñ—à–Ω—ñ—Å—Ç—å: {success_rate:.1f}%")
                if avg_time > 0:
                    logger.info(
                        f"      –°–µ—Ä. —á–∞—Å: {avg_time:.2f}—Å (–º—ñ–Ω: {stats['fastest']:.2f}—Å, –º–∞–∫—Å: {stats['slowest']:.2f}—Å)")
                logger.info(f"      –û—Å—Ç–∞–Ω–Ω—î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è: {stats['last_used']}")
                logger.info("-" * 70)

        # –û—Å—Ç–∞–Ω–Ω—ñ –ø–æ–º–∏–ª–∫–∏
        if self.recent_failures:
            logger.warning("‚ö†Ô∏è –û—Å—Ç–∞–Ω–Ω—ñ –ø–æ–º–∏–ª–∫–∏:")
            for failure in list(self.recent_failures)[-5:]:
                logger.warning(f"   ‚Ä¢ {failure['time']} - {failure['proxy']}")

        logger.info("=" * 70)

    def get_working_proxies_count(self) -> int:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ"""
        return len(VALID_PROXY_LIST)

    def get_fastest_proxy(self) -> str:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –Ω–∞–π—à–≤–∏–¥—à–µ –ø—Ä–æ–∫—Å—ñ"""
        fastest = None
        fastest_time = float('inf')

        for proxy, stats in self.usage_stats.items():
            if stats['used'] > 0 and stats['fastest'] < fastest_time:
                fastest_time = stats['fastest']
                fastest = proxy

        return fastest

================================================================================
üìÑ –§–ê–ô–õ: src\proxy_utils.py
================================================================================

"""
–£—Ç–∏–ª—ñ—Ç–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –ø—Ä–æ–∫—Å—ñ
–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è, –≤–∞–ª—ñ–¥–∞—Ü—ñ—è, —Ä–æ—Ç–∞—Ü—ñ—è —Ç–∞ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥
"""

import asyncio
import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict

from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from loguru import logger

# –î–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏—Ö —Ç–µ—Å—Ç—ñ–≤
import requests
from concurrent.futures import ThreadPoolExecutor


@dataclass
class ProxyTestResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ"""
    server: str
    is_working: bool
    response_time: float
    ip: str = None
    country: str = None
    error: str = None
    timestamp: str = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()


class ProxyTester:
    """
    –ö–ª–∞—Å –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∞—Ü–µ–∑–¥–∞—Ç–Ω–æ—Å—Ç—ñ –ø—Ä–æ–∫—Å—ñ
    –ü—ñ–¥—Ç—Ä–∏–º—É—î —è–∫ Playwright —Ç–∞–∫ —ñ requests
    """

    def __init__(self, timeout: int = 10):
        """
        Args:
            timeout: –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–µ—Å—Ç—É
        """
        self.timeout = timeout
        self.test_urls = [
            "http://httpbin.org/ip",
            "http://httpbin.org/get",
            "http://api.ipify.org?format=json"
        ]

    async def test_with_playwright(self, proxy: dict) -> ProxyTestResult:
        """
        –¢–µ—Å—Ç—É—î –ø—Ä–æ–∫—Å—ñ —á–µ—Ä–µ–∑ Playwright (–Ω–∞–π—Ç–æ—á–Ω—ñ—à–µ, –±–æ –µ–º—É–ª—é—î –±—Ä–∞—É–∑–µ—Ä)

        Args:
            proxy: –°–ª–æ–≤–Ω–∏–∫ –∑ –∫–æ–Ω—Ñ—ñ–≥–æ–º –ø—Ä–æ–∫—Å—ñ {'server': 'http://ip:port', ...}

        Returns:
            ProxyTestResult –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç—É
        """
        server = proxy.get('server', 'unknown')
        start_time = time.time()

        try:
            logger.debug(f"üß™ –¢–µ—Å—Ç—É—î–º–æ –ø—Ä–æ–∫—Å—ñ (Playwright): {server}")

            async with async_playwright() as p:
                # –ó–∞–ø—É—Å–∫–∞—î–º–æ –±—Ä–∞—É–∑–µ—Ä –∑ –ø—Ä–æ–∫—Å—ñ
                browser = await p.chromium.launch(
                    headless=True,  # –ù–µ –ø–æ–∫–∞–∑—É—î–º–æ –≤—ñ–∫–Ω–æ
                    proxy=proxy
                )

                # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑ —Ç–∞–π–º–∞—É—Ç–æ–º
                context = await browser.new_context()
                page = await context.new_page()

                # –¢–µ—Å—Ç 1: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è —Ç–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è IP
                try:
                    await page.goto("http://httpbin.org/ip",
                                    timeout=self.timeout * 1000)
                    content = await page.text_content("body")

                    # –ü–∞—Ä—Å–∏–º–æ JSON –≤—ñ–¥–ø–æ–≤—ñ–¥—å
                    import json
                    ip_data = json.loads(content)
                    current_ip = ip_data.get('origin', 'unknown')

                except PlaywrightTimeoutError:
                    await browser.close()
                    return ProxyTestResult(
                        server=server,
                        is_working=False,
                        response_time=time.time() - start_time,
                        error="Timeout - –ø—Ä–æ–∫—Å—ñ –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î"
                    )

                # –¢–µ—Å—Ç 2: –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —à–≤–∏–¥–∫–æ—Å—Ç—ñ
                speed_start = time.time()
                await page.goto("http://httpbin.org/delay/1",
                                timeout=self.timeout * 1000)
                speed_end = time.time()
                response_time = speed_end - speed_start

                # –ó–∞–∫—Ä–∏–≤–∞—î–º–æ –±—Ä–∞—É–∑–µ—Ä
                await browser.close()

                # –¢–µ—Å—Ç 3: –°–ø—Ä–æ–±–∞ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –∫—Ä–∞—ó–Ω—É (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
                country = await self._get_country_from_ip(current_ip)

                return ProxyTestResult(
                    server=server,
                    is_working=True,
                    response_time=response_time,
                    ip=current_ip,
                    country=country
                )

        except Exception as e:
            error_msg = str(e)
            logger.debug(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–æ–∫—Å—ñ {server}: {error_msg[:100]}")

            return ProxyTestResult(
                server=server,
                is_working=False,
                response_time=time.time() - start_time,
                error=error_msg[:200]
            )

    def test_with_requests(self, proxy: dict) -> ProxyTestResult:
        """
        –®–≤–∏–¥–∫–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ —á–µ—Ä–µ–∑ requests (–±–µ–∑ –±—Ä–∞—É–∑–µ—Ä–∞)
        –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –¥–ª—è –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ—ó —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó

        Args:
            proxy: –°–ª–æ–≤–Ω–∏–∫ –∑ –∫–æ–Ω—Ñ—ñ–≥–æ–º –ø—Ä–æ–∫—Å—ñ
        """
        server = proxy.get('server', 'unknown')
        start_time = time.time()

        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ —Ñ–æ—Ä–º–∞—Ç Playwright -> requests
            proxies = {
                "http": server,
                "https": server
            }

            # –î–æ–¥–∞—î–º–æ –∞–≤—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—é —è–∫—â–æ —î
            auth = None
            if "username" in proxy and "password" in proxy:
                from requests.auth import HTTPProxyAuth
                auth = HTTPProxyAuth(proxy["username"], proxy["password"])

            response = requests.get(
                "http://httpbin.org/ip",
                proxies=proxies,
                auth=auth,
                timeout=self.timeout
            )

            response_time = time.time() - start_time
            ip_data = response.json()
            current_ip = ip_data.get('origin', 'unknown')

            return ProxyTestResult(
                server=server,
                is_working=True,
                response_time=response_time,
                ip=current_ip
            )

        except Exception as e:
            return ProxyTestResult(
                server=server,
                is_working=False,
                response_time=time.time() - start_time,
                error=str(e)[:200]
            )

    async def _get_country_from_ip(self, ip: str) -> str:
        """–í–∏–∑–Ω–∞—á–∞—î –∫—Ä–∞—ó–Ω—É –∑–∞ IP (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)"""
        try:
            response = requests.get(f"http://ip-api.com/json/{ip}", timeout=2)
            data = response.json()
            return data.get('country', 'unknown')
        except:
            return 'unknown'

    async def test_batch(self, proxy_list: List[dict],
                         max_workers: int = 5,
                         use_playwright: bool = False) -> List[ProxyTestResult]:
        """
        –¢–µ—Å—Ç—É—î —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ

        Args:
            proxy_list: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
            max_workers: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö —Ç–µ—Å—Ç—ñ–≤
            use_playwright: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ Playwright (–ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ, –∞–ª–µ —Ç–æ—á–Ω—ñ—à–µ)

        Returns:
            List[ProxyTestResult]: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
        """
        logger.info(f"üß™ –ü–æ—á–∞—Ç–æ–∫ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è {len(proxy_list)} –ø—Ä–æ–∫—Å—ñ...")

        results = []

        if use_playwright:
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑ Playwright
            semaphore = asyncio.Semaphore(max_workers)

            async def test_with_limit(proxy):
                async with semaphore:
                    return await self.test_with_playwright(proxy)

            tasks = [test_with_limit(proxy) for proxy in proxy_list]
            results = await asyncio.gather(*tasks)
        else:
            # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –∑ requests (—à–≤–∏–¥—à–µ)
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                results = list(executor.map(
                    lambda p: self.test_with_requests(p),
                    proxy_list
                ))

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        working = [r for r in results if r.is_working]
        logger.success(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ {len(working)}/{len(proxy_list)} —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ")

        return results


class ProxyManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–∫—Å—ñ –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—é —Ä–æ—Ç–∞—Ü—ñ—î—é —Ç–∞ —á–æ—Ä–Ω–∏–º —Å–ø–∏—Å–∫–æ–º
    """

    def __init__(self, proxy_list: List[dict] = None):
        """
        Args:
            proxy_list: –ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ
        """
        self.all_proxies = proxy_list or []
        self.working_proxies = []
        self.blacklist = []  # –ü—Ä–æ–∫—Å—ñ, —è–∫—ñ –Ω–µ –ø—Ä–∞—Ü—é—é—Ç—å
        self.current_index = 0
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "rotations": 0
        }

    def add_proxy(self, proxy: dict):
        """–î–æ–¥–∞—î –ø—Ä–æ–∫—Å—ñ –¥–æ –ø—É–ª—É"""
        self.all_proxies.append(proxy)

    def remove_proxy(self, proxy: dict):
        """–í–∏–¥–∞–ª—è—î –ø—Ä–æ–∫—Å—ñ –∑ –ø—É–ª—É"""
        if proxy in self.all_proxies:
            self.all_proxies.remove(proxy)
        if proxy in self.working_proxies:
            self.working_proxies.remove(proxy)

    def mark_failed(self, proxy: dict):
        """–ü–æ–∑–Ω–∞—á–∞—î –ø—Ä–æ–∫—Å—ñ —è–∫ –Ω–µ—Å–ø—Ä–∞–≤–Ω–µ —ñ –¥–æ–¥–∞—î –≤ —á–æ—Ä–Ω–∏–π —Å–ø–∏—Å–æ–∫"""
        self.stats["failed_requests"] += 1
        self.remove_proxy(proxy)
        if proxy not in self.blacklist:
            self.blacklist.append(proxy)
            logger.warning(f"‚õî –ü—Ä–æ–∫—Å—ñ –¥–æ–¥–∞–Ω–æ –¥–æ —á–æ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫—É: {proxy.get('server')}")

    def mark_success(self, proxy: dict):
        """–ü–æ–∑–Ω–∞—á–∞—î —É—Å–ø—ñ—à–Ω–∏–π –∑–∞–ø–∏—Ç"""
        self.stats["successful_requests"] += 1

    def get_next_proxy(self) -> Optional[dict]:
        """
        –ü–æ–≤–µ—Ä—Ç–∞—î –Ω–∞—Å—Ç—É–ø–Ω–µ –ø—Ä–æ–∫—Å—ñ –∑ —Ä–æ—Ç–∞—Ü—ñ—î—é (round-robin)

        Returns:
            dict: –ö–æ–Ω—Ñ—ñ–≥ –ø—Ä–æ–∫—Å—ñ –∞–±–æ None —è–∫—â–æ –Ω–µ–º–∞—î —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ
        """
        if not self.working_proxies:
            # –Ø–∫—â–æ –Ω–µ–º–∞—î –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ —Ä–æ–±–æ—á–∏—Ö, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤—Å—ñ
            available = self.all_proxies
        else:
            available = self.working_proxies

        if not available:
            logger.error("‚ùå –ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –ø—Ä–æ–∫—Å—ñ!")
            return None

        self.current_index = (self.current_index + 1) % len(available)
        proxy = available[self.current_index]
        self.stats["rotations"] += 1
        self.stats["total_requests"] += 1

        logger.debug(f"üîÑ –†–æ—Ç–∞—Ü—ñ—è: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–æ–∫—Å—ñ #{self.current_index + 1}")
        return proxy.copy()  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –∫–æ–ø—ñ—é

    async def verify_and_update(self, tester: ProxyTester):
        """
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î –≤—Å—ñ –ø—Ä–æ–∫—Å—ñ —ñ –æ–Ω–æ–≤–ª—é—î —Å–ø–∏—Å–æ–∫ —Ä–æ–±–æ—á–∏—Ö

        Args:
            tester: –ï–∫–∑–µ–º–ø–ª—è—Ä ProxyTester
        """
        if not self.all_proxies:
            return

        logger.info("üîç –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –≤—Å—ñ—Ö –ø—Ä–æ–∫—Å—ñ...")
        results = await tester.test_batch(self.all_proxies, use_playwright=True)

        self.working_proxies = [
            r.server for r in results if r.is_working
        ]

        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∫—Å—ñ: {len(self.working_proxies)} —Ä–æ–±–æ—á–∏—Ö, "
                    f"{len(self.blacklist)} –≤ —á–æ—Ä–Ω–æ–º—É —Å–ø–∏—Å–∫—É")

    def get_stats(self) -> dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ"""
        return {
            **self.stats,
            "total_proxies": len(self.all_proxies),
            "working_proxies": len(self.working_proxies),
            "blacklisted": len(self.blacklist),
            "success_rate": self.stats["successful_requests"] / max(self.stats["total_requests"], 1)
        }


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ
async def find_working_proxies(proxy_list: List[dict],
                               min_speed: float = 5.0) -> List[dict]:
    """
    –®–≤–∏–¥–∫–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–æ–±–æ—á—ñ –ø—Ä–æ–∫—Å—ñ –∑—ñ —Å–ø–∏—Å–∫—É

    Args:
        proxy_list: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
        min_speed: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

    Returns:
        List[dict]: –°–ø–∏—Å–æ–∫ —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ
    """
    tester = ProxyTester(timeout=5)

    # –°–ø–æ—á–∞—Ç–∫—É —à–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ requests
    logger.info("üöÄ –®–≤–∏–¥–∫–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ...")
    quick_results = tester.test_batch(proxy_list, use_playwright=False)

    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ä–æ–±–æ—á—ñ
    working_quick = []
    for result in quick_results:
        if isinstance(result, ProxyTestResult) and result.is_working:
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –ø—Ä–æ–∫—Å—ñ
            for proxy in proxy_list:
                if proxy.get('server') == result.server:
                    working_quick.append(proxy)
                    break

    logger.info(f"‚ö° –ó–Ω–∞–π–¥–µ–Ω–æ {len(working_quick)} –ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω–æ —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ")

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —ó—Ö —á–µ—Ä–µ–∑ Playwright (—Ç–æ—á–Ω—ñ—à–µ)
    if working_quick:
        logger.info("üé≠ –§—ñ–Ω–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–µ—Ä–µ–∑ Playwright...")
        final_results = await tester.test_batch(working_quick, use_playwright=True)

        working_final = []
        for result in final_results:
            if result.is_working and result.response_time < min_speed:
                for proxy in working_quick:
                    if proxy.get('server') == result.server:
                        working_final.append(proxy)
                        logger.success(f"‚úÖ {result.server} - {result.response_time:.2f}—Å - {result.ip}")
                        break

        return working_final

    return []


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ –≤ —Ñ–∞–π–ª
def save_working_proxies(proxies: List[dict], filename: str = "working_proxies.json"):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î —Å–ø–∏—Å–æ–∫ —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ –≤ JSON —Ñ–∞–π–ª

    Args:
        proxies: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ
        filename: –Ü–º'—è —Ñ–∞–π–ª—É
    """
    filepath = Path("data") / filename
    filepath.parent.mkdir(exist_ok=True)

    # –í–∏–¥–∞–ª—è—î–º–æ –ø–∞—Ä–æ–ª—ñ –ø–µ—Ä–µ–¥ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º!
    safe_proxies = []
    for p in proxies:
        safe_p = {"server": p["server"]}
        if "username" in p:
            safe_p["username"] = p["username"][:3] + "***"
        safe_p["has_auth"] = "password" in p
        safe_proxies.append(safe_p)

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(safe_proxies, f, indent=2, ensure_ascii=False)

    logger.success(f"üíæ –†–æ–±–æ—á—ñ –ø—Ä–æ–∫—Å—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {filepath}")


# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ –≤—Å—ñ—Ö –ø—Ä–æ–∫—Å—ñ
async def benchmark_proxies(proxy_list: List[dict]) -> List[Tuple[dict, float]]:
    """
    –¢–µ—Å—Ç—É—î —à–≤–∏–¥–∫—ñ—Å—Ç—å –≤—Å—ñ—Ö –ø—Ä–æ–∫—Å—ñ —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π —Å–ø–∏—Å–æ–∫

    Args:
        proxy_list: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ

    Returns:
        List[Tuple[dict, float]]: –í—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω—ñ –∑–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—é (—à–≤–∏–¥—à—ñ –ø–µ—Ä—à—ñ)
    """
    tester = ProxyTester()
    results = await tester.test_batch(proxy_list, use_playwright=True)

    proxy_speed = []
    for result in results:
        if result.is_working:
            for proxy in proxy_list:
                if proxy.get('server') == result.server:
                    proxy_speed.append((proxy, result.response_time))
                    break

    # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—é
    proxy_speed.sort(key=lambda x: x[1])

    logger.info("üìä –¢–û–ü-5 –Ω–∞–π—à–≤–∏–¥—à–∏—Ö –ø—Ä–æ–∫—Å—ñ:")
    for i, (proxy, speed) in enumerate(proxy_speed[:5], 1):
        logger.info(f"   {i}. {proxy['server']} - {speed:.2f}—Å")

    return proxy_speed


# –ï–∫—Å–ø–æ—Ä—Ç—É—î–º–æ –æ—Å–Ω–æ–≤–Ω—ñ –∫–ª–∞—Å–∏ —Ç–∞ —Ñ—É–Ω–∫—Ü—ñ—ó
__all__ = [
    'ProxyTester',
    'ProxyManager',
    'ProxyTestResult',
    'find_working_proxies',
    'save_working_proxies',
    'benchmark_proxies'
]

================================================================================
üìÑ –§–ê–ô–õ: src\scraper.py
================================================================================

# src/scraper.py
import asyncio
import random
import time
from src.client import BrowserClient
from src.parser import OLXParser
from src.models import OLXItem
from loguru import logger
from src.utils import human_delay, smooth_scroll, human_mouse_move
from src.settings import BASE_DELAY, VALID_PROXY_LIST
from src.state_manager import StateManager
from src.exporter import Exporter
from src.stealth import ManualStealth, get_stealth_for_site
from src.proxy_monitor import ProxyMonitor
from src.semaphore_manager import get_semaphore, AsyncTaskGroup


class Scraper:
    def __init__(self, max_items: int = 50, proxy: dict = None,
                 stealth: ManualStealth = None, site_name: str = "OLX",
                 max_concurrent: int = 3):

        # ===== –ü–ï–†–ï–í–Ü–†–ö–ê –ü–†–û–ö–°–Ü =====
        if proxy is None and not VALID_PROXY_LIST:
            logger.critical("=" * 60)
            logger.critical("üî¥ –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: –°–ö–†–ê–ü–ï–† –ù–ï –ú–û–ñ–ï –ü–†–ê–¶–Æ–í–ê–¢–ò –ë–ï–ó –ü–†–û–ö–°–Ü!")
            logger.critical("=" * 60)
            logger.critical("üõ°Ô∏è –ó–ê–•–ò–°–¢: –ü—Ä–æ–≥—Ä–∞–º–∞ –±—É–¥–µ –∑—É–ø–∏–Ω–µ–Ω–∞")
            logger.critical("üí° –†—ñ—à–µ–Ω–Ω—è: –î–æ–¥–∞–π—Ç–µ –ø—Ä–æ–∫—Å—ñ –≤ config.yaml")
            logger.critical("=" * 60)
            raise Exception("Scraper –≤–∏–º–∞–≥–∞—î –ø—Ä–æ–∫—Å—ñ –¥–ª—è —Ä–æ–±–æ—Ç–∏")

        self.client = BrowserClient(proxy=proxy)
        self.parser = OLXParser()
        self.max_items = max_items
        self.results: list[OLXItem] = []
        self._lock = asyncio.Lock()
        self.state_manager = StateManager()
        self.stealth = stealth or get_stealth_for_site('ukraine')

        # ===== SEMAPHORE –î–õ–Ø –ö–û–ù–¢–†–û–õ–Æ –ù–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø =====
        self.site_name = site_name
        self.max_concurrent = max_concurrent
        self.semaphore = get_semaphore(site_name, max_concurrent)
        self.page_semaphore = asyncio.Semaphore(max_concurrent)  # –î–ª—è —Å—Ç–æ—Ä—ñ–Ω–æ–∫

        # –ú–æ–Ω—ñ—Ç–æ—Ä –ø—Ä–æ–∫—Å—ñ
        self.proxy_monitor = ProxyMonitor()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stealth_used = 0
        self.behavior_imitated = 0
        self.pages_processed = 0
        self.start_time = None
        self.total_pages = 0
        self.failed_pages = 0

    async def _simulate_human_behavior(self, page):
        """–Ü–º—ñ—Ç–∞—Ü—ñ—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –ª—é–¥–∏–Ω–∏"""
        try:
            # –í–∏–ø–∞–¥–∫–æ–≤–∞ –ø—Ä–æ–∫—Ä—É—Ç–∫–∞
            scroll_amount = random.randint(200, 500)
            await page.evaluate(f"window.scrollBy(0, {scroll_amount})")
            await asyncio.sleep(random.uniform(0.5, 1.5))

            # –í–∏–ø–∞–¥–∫–æ–≤–∏–π —Ä—É—Ö –º–∏—à—ñ
            viewport = page.viewport_size
            if viewport:
                x = random.randint(100, viewport['width'] - 100)
                y = random.randint(100, viewport['height'] - 100)
                await page.mouse.move(x, y, steps=random.randint(10, 20))

                # –Ü–Ω–æ–¥—ñ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ —Ä—É—Ö–∏
                if random.random() < 0.3:
                    for _ in range(random.randint(2, 4)):
                        new_x = x + random.randint(-50, 50)
                        new_y = y + random.randint(-50, 50)
                        await page.mouse.move(new_x, new_y, steps=5)
                        await asyncio.sleep(random.uniform(0.1, 0.2))

            logger.debug(f"[{self.site_name}] üñ±Ô∏è –Ü–º—ñ—Ç–∞—Ü—ñ—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –ª—é–¥–∏–Ω–∏")
            return True
        except Exception as e:
            logger.debug(f"[{self.site_name}] –ù–µ –≤–¥–∞–ª–æ—Å—è —ñ–º—ñ—Ç—É–≤–∞—Ç–∏ –ø–æ–≤–µ–¥—ñ–Ω–∫—É: {e}")
            return False

    async def scrape_page(self, url: str, index: int, proxy_override: dict = None) -> str | None:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –æ–¥–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ—Ä–µ–∑ Semaphore
        """
        if len(self.results) >= self.max_items:
            logger.info(f"[{self.site_name}] üéØ –î–æ—Å—è–≥–Ω—É—Ç–æ –ª—ñ–º—ñ—Ç—É –≤ {self.max_items} –æ–≥–æ–ª–æ—à–µ–Ω—å")
            return None

        # ===== –ö–û–ù–¢–†–û–õ–¨ –ù–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ß–ï–†–ï–ó SEMAPHORE =====
        async with self.page_semaphore:
            logger.debug(
                f"[{self.site_name}] üîë –û—Ç—Ä–∏–º–∞–Ω–æ –¥–æ—Å—Ç—É–ø –¥–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ #{index} (–∞–∫—Ç–∏–≤–Ω–∏—Ö: {self.max_concurrent - self.page_semaphore._value}/{self.max_concurrent})")

            return await self._scrape_page_internal(url, index, proxy_override)

    async def _scrape_page_internal(self, url: str, index: int, proxy_override: dict = None) -> str | None:
        """–í–Ω—É—Ç—Ä—ñ—à–Ω—ñ–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""

        # ===== –ü–ï–†–ï–í–Ü–†–ö–ê –ü–†–û–ö–°–Ü =====
        if not VALID_PROXY_LIST:
            logger.critical(f"[{self.site_name}] üî¥ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ù–ï–ú–ê–Ñ –ü–†–û–ö–°–Ü!")
            return "ERROR_SIGNAL"

        # –í–∏–±—ñ—Ä –ø—Ä–æ–∫—Å—ñ
        safe_index = (index - 1) % len(VALID_PROXY_LIST)
        current_proxy = proxy_override or VALID_PROXY_LIST[safe_index]
        logger.info(f"[{self.site_name}] üîå [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ü—Ä–æ–∫—Å—ñ: {current_proxy['server']}")

        # –í–∏–ø–∞–¥–∫–æ–≤–∏–π User-Agent
        current_ua = self.client.get_random_ua()

        # ===== –°–¢–í–û–†–ï–ù–ù–Ø –ö–û–ù–¢–ï–ö–°–¢–£ =====
        try:
            if self.stealth:
                context = await self.stealth.create_context(self.client.browser)
                logger.debug(f"[{self.site_name}] üïµÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ —á–µ—Ä–µ–∑ —Å—Ç–µ–ª—Å")
            else:
                context = await self.client.browser.new_context(
                    user_agent=current_ua,
                    proxy=current_proxy,
                    viewport={"width": random.choice([1366, 1440, 1536, 1920]),
                              "height": random.choice([768, 900, 864, 1080])}
                )
        except Exception as e:
            logger.error(f"[{self.site_name}] ‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É: {e}")
            return "ERROR_SIGNAL"

        page = await context.new_page()

        # ===== –ó–ê–°–¢–û–°–£–í–ê–ù–ù–Ø –°–¢–ï–õ–°–£ =====
        if self.stealth:
            try:
                await self.stealth.apply_to_page(page)
                self.stealth_used += 1
                logger.debug(f"[{self.site_name}] üïµÔ∏è –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–æ")
            except Exception as e:
                logger.warning(f"[{self.site_name}] ‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —Å—Ç–µ–ª—Å—É: {e}")

        try:
            logger.info(f"[{self.site_name}] üöÄ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {url}")

            # –Ü–º—ñ—Ç–∞—Ü—ñ—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –ª—é–¥–∏–Ω–∏
            if random.random() < 0.7:
                if await self._simulate_human_behavior(page):
                    self.behavior_imitated += 1

            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            # –ü–∞—Ä—Å–∏–º–æ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
            new_items = await self.parser.parse_listings(page)
            next_page_url = await self.parser.get_next_page(page)

            # –û–Ω–æ–≤–ª—é—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            async with self._lock:
                added = self._update_results(new_items)
                count = len(self.results)

            if added > 0:
                logger.success(f"[{self.site_name}] ‚úÖ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] +{added} –æ–≥–æ–ª–æ—à–µ–Ω—å (–í—Å—å–æ–≥–æ: {count})")
                self.total_pages += 1
            else:
                logger.info(f"[{self.site_name}] ‚ÑπÔ∏è [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ù–æ–≤–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
            if next_page_url:
                self.state_manager.save_checkpoint(next_page_url, count)

            # –í–∏–ø–∞–¥–∫–æ–≤–∞ –∑–∞—Ç—Ä–∏–º–∫–∞
            delay = random.uniform(BASE_DELAY[0], BASE_DELAY[1])
            logger.debug(f"[{self.site_name}] üí§ –ó–∞—Ç—Ä–∏–º–∫–∞ {delay:.1f}—Å –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–æ—é —Å—Ç–æ—Ä—ñ–Ω–∫–æ—é")
            await asyncio.sleep(delay)

            return next_page_url

        except Exception as e:
            logger.error(f"[{self.site_name}] ‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ #{index}: {e}")
            self.failed_pages += 1
            return "ERROR_SIGNAL"
        finally:
            await context.close()
            self.pages_processed += 1

    async def scrape_page_with_retry(self, url: str, index: int, max_retries: int = 3):
        """–°–ø—Ä–æ–±–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É –∑ —Ä–æ—Ç–∞—Ü—ñ—î—é –ø—Ä–æ–∫—Å—ñ"""

        if not self.proxy_monitor or not self.proxy_monitor.check_proxy_health():
            logger.critical(f"[{self.site_name}] üî¥ –í–°–Ü –ü–†–û–ö–°–Ü –ú–ï–†–¢–í–Ü!")
            return "ERROR_SIGNAL"

        for attempt in range(max_retries):
            start_time = time.time()
            current_proxy = None

            try:
                if not VALID_PROXY_LIST:
                    logger.critical(f"[{self.site_name}] üî¥ –ü–†–û–ö–°–Ü –ó–ê–ö–Ü–ù–ß–ò–õ–ò–°–¨!")
                    if self.proxy_monitor:
                        self.proxy_monitor.print_stats()
                    return "ERROR_SIGNAL"

                proxy_index = (index + attempt) % len(VALID_PROXY_LIST)
                current_proxy = VALID_PROXY_LIST[proxy_index]

                logger.info(f"[{self.site_name}] üîÑ –°–ø—Ä–æ–±–∞ {attempt + 1}/{max_retries} –¥–ª—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ #{index}")
                logger.info(f"[{self.site_name}] üîå –ü—Ä–æ–∫—Å—ñ: {current_proxy['server']}")

                result = await asyncio.wait_for(
                    self.scrape_page(url, index, current_proxy),
                    timeout=45
                )

                response_time = time.time() - start_time
                if self.proxy_monitor and current_proxy:
                    self.proxy_monitor.log_proxy_usage(
                        current_proxy['server'],
                        success=True,
                        response_time=response_time
                    )

                if result != "ERROR_SIGNAL":
                    if attempt > 0:
                        logger.success(f"[{self.site_name}] ‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index} –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑ {attempt + 1} —Å–ø—Ä–æ–±–∏")
                    return result

            except asyncio.TimeoutError:
                logger.warning(f"[{self.site_name}] ‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ #{index}")
                if self.proxy_monitor and current_proxy:
                    self.proxy_monitor.log_proxy_usage(current_proxy['server'], success=False)

            except Exception as e:
                if self.proxy_monitor and current_proxy:
                    self.proxy_monitor.log_proxy_usage(current_proxy['server'], success=False)

                if "ERR_PROXY_CONNECTION_FAILED" in str(e) and current_proxy:
                    logger.warning(f"[{self.site_name}] üî¥ –ü—Ä–æ–∫—Å—ñ –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î: {current_proxy['server']}")
                    if VALID_PROXY_LIST and current_proxy in VALID_PROXY_LIST:
                        VALID_PROXY_LIST.remove(current_proxy)
                        logger.warning(
                            f"[{self.site_name}] üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ –º–µ—Ä—Ç–≤–µ –ø—Ä–æ–∫—Å—ñ. –ó–∞–ª–∏—à–∏–ª–æ—Å—å: {len(VALID_PROXY_LIST)}")

                        if not VALID_PROXY_LIST and self.proxy_monitor:
                            logger.critical(f"[{self.site_name}] üî¥ –í–°–Ü –ü–†–û–ö–°–Ü –í–ò–î–ê–õ–ï–ù–û!")
                            self.proxy_monitor.print_stats()
                            return "ERROR_SIGNAL"
                else:
                    logger.warning(f"[{self.site_name}] ‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞: {str(e)[:100]}")

            wait_time = 2 ** attempt
            logger.info(f"[{self.site_name}] üí§ –û—á—ñ–∫—É–≤–∞–Ω–Ω—è {wait_time}—Å")
            await asyncio.sleep(wait_time)

        logger.error(f"[{self.site_name}] ‚ùå –í—Å—ñ {max_retries} —Å–ø—Ä–æ–± –¥–ª—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ #{index} –Ω–µ –≤–¥–∞–ª–∏—Å—è")
        return "ERROR_SIGNAL"

    async def run(self, start_url: str):
        """–û—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫—É"""
        self.start_time = time.time()
        await self.client.start()

        checkpoint_data = self.state_manager.load_checkpoint()
        current_url = start_url

        if isinstance(checkpoint_data, dict):
            current_url = checkpoint_data.get("last_url", start_url)
        elif isinstance(checkpoint_data, str):
            current_url = checkpoint_data

        if current_url != start_url:
            logger.info(f"[{self.site_name}] ‚ôªÔ∏è –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ —á–µ–∫–ø–æ—ó–Ω—Ç–∞: {current_url}")

        page_index = 1
        start_time = time.time()

        try:
            while current_url and len(self.results) < self.max_items:
                result = await self.scrape_page_with_retry(current_url, page_index)

                if result == "ERROR_SIGNAL":
                    logger.warning(f"[{self.site_name}] ‚ö†Ô∏è –ü–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è. –ß–µ–∫–ø–æ—ó–Ω—Ç: {current_url}")
                    break

                current_url = result
                page_index += 1

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            elapsed_time = time.time() - start_time
            logger.info("=" * 60)
            logger.info(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–õ–Ø {self.site_name}")
            logger.info("=" * 60)
            logger.info(f"üìÑ –°—Ç–æ—Ä—ñ–Ω–æ–∫ —É—Å–ø—ñ—à–Ω–æ: {self.total_pages}")
            logger.info(f"‚ùå –°—Ç–æ—Ä—ñ–Ω–æ–∫ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏: {self.failed_pages}")
            logger.info(f"üì¶ –û–≥–æ–ª–æ—à–µ–Ω—å: {len(self.results)}")
            logger.info(f"‚è±Ô∏è –ß–∞—Å: {elapsed_time:.1f} —Å–µ–∫")
            if elapsed_time > 0:
                logger.info(f"‚ö° –®–≤–∏–¥–∫—ñ—Å—Ç—å: {len(self.results) / elapsed_time:.2f} –æ–≥–æ–ª./—Å–µ–∫")
            logger.info(f"üïµÔ∏è –°—Ç–µ–ª—Å: {self.stealth_used} —Ä–∞–∑—ñ–≤")
            logger.info(f"üñ±Ô∏è –Ü–º—ñ—Ç–∞—Ü—ñ–π: {self.behavior_imitated} —Ä–∞–∑—ñ–≤")
            logger.info("=" * 60)

            if self.proxy_monitor:
                self.proxy_monitor.print_stats()

            if len(self.results) >= self.max_items or current_url is None:
                self.state_manager.clear_checkpoint()

            return self.results

        except Exception as e:
            logger.critical(f"[{self.site_name}] üí• –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self.results
        finally:
            await self.client.stop()

    def _update_results(self, new_items: list[OLXItem]) -> int:
        """–û–Ω–æ–≤–ª—é—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏, —É–Ω–∏–∫–∞—é—á–∏ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤"""
        existing_urls = {item.url for item in self.results}
        added = 0

        for item in new_items:
            if item.url not in existing_urls and len(self.results) < self.max_items:
                self.results.append(item)
                existing_urls.add(item.url)
                added += 1
                Exporter.append_to_csv(item, filename=f"{self.site_name.lower()}_results.csv")

        return added


# ===== –ö–õ–ê–° –î–õ–Ø –ü–ê–†–ê–õ–ï–õ–¨–ù–û–ì–û –ó–ê–ü–£–°–ö–£ –ö–Ü–õ–¨–ö–û–• –°–ê–ô–¢–Ü–í =====

class MultiSiteScraper:
    """
    –ö–ª–∞—Å –¥–ª—è –æ–¥–Ω–æ—á–∞—Å–Ω–æ–≥–æ —Å–∫—Ä–∞–ø—ñ–Ω–≥—É –∫—ñ–ª—å–∫–æ—Ö —Å–∞–π—Ç—ñ–≤ –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    """

    def __init__(self, max_concurrent_total: int = 3):
        """
        Args:
            max_concurrent_total: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–∞–π—Ç—ñ–≤ –æ–¥–Ω–æ—á–∞—Å–Ω–æ
        """
        self.max_concurrent_total = max_concurrent_total
        self.total_semaphore = asyncio.Semaphore(max_concurrent_total)
        self.scrapers = []
        self.results = {}

    def add_site(self, site_name: str, url: str, max_items: int = 50,
                 site_max_concurrent: int = 2):
        """
        –î–æ–¥–∞—Ç–∏ —Å–∞–π—Ç –¥–ª—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É

        Args:
            site_name: –ù–∞–∑–≤–∞ —Å–∞–π—Ç—É
            url: URL –¥–ª—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É
            max_items: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–≥–æ–ª–æ—à–µ–Ω—å
            site_max_concurrent: –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è —Ü—å–æ–≥–æ —Å–∞–π—Ç—É
        """
        scraper = Scraper(
            max_items=max_items,
            site_name=site_name,
            max_concurrent=site_max_concurrent
        )
        self.scrapers.append({
            'name': site_name,
            'url': url,
            'scraper': scraper
        })

    async def _run_site(self, site_info: dict) -> tuple:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç–∏ —Å–∫—Ä–∞–ø—ñ–Ω–≥ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç—É –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        """
        async with self.total_semaphore:
            logger.info(f"üöÄ –°—Ç–∞—Ä—Ç —Å–∫—Ä–∞–ø—ñ–Ω–≥—É –¥–ª—è {site_info['name']}")
            try:
                results = await site_info['scraper'].run(site_info['url'])
                logger.success(f"‚úÖ {site_info['name']} –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(results)} –æ–≥–æ–ª–æ—à–µ–Ω—å")
                return site_info['name'], results
            except Exception as e:
                logger.error(f"‚ùå {site_info['name']} –ø–æ–º–∏–ª–∫–∞: {e}")
                return site_info['name'], None

    async def run_all(self) -> dict:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç–∏ —Å–∫—Ä–∞–ø—ñ–Ω–≥ –≤—Å—ñ—Ö –¥–æ–¥–∞–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
        """
        logger.info(f"üöÄ –ó–ê–ü–£–°–ö {len(self.scrapers)} –°–ê–ô–¢–Ü–í –ü–ê–†–ê–õ–ï–õ–¨–ù–û")
        logger.info(f"üìä –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ—á–∞—Å–Ω–æ: {self.max_concurrent_total}")

        tasks = [self._run_site(site) for site in self.scrapers]
        results_list = await asyncio.gather(*tasks)

        self.results = dict(results_list)

        # –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info("=" * 60)
        logger.info("üìä –ó–ê–ì–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        logger.info("=" * 60)
        total = 0
        for name, results in self.results.items():
            if results:
                count = len(results)
                total += count
                logger.info(f"   ‚Ä¢ {name}: {count} –æ–≥–æ–ª–æ—à–µ–Ω—å")
        logger.info(f"   ‚Ä¢ –í–°–¨–û–ì–û: {total} –æ–≥–æ–ª–æ—à–µ–Ω—å")
        logger.info("=" * 60)

        return self.results

================================================================================
üìÑ –§–ê–ô–õ: src\semaphore_manager.py
================================================================================

# src/semaphore_manager.py
"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—é –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤ (Semaphore)
–î–æ–∑–≤–æ–ª—è—î —É–Ω–∏–∫–Ω—É—Ç–∏ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è —Ç–∞ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–∞–π—Ç—ñ–≤
"""

import asyncio
from loguru import logger
from typing import Optional, Dict, Any
import time


class SemaphoreManager:
    """
    –ö–ª–∞—Å –¥–ª—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏–º–∏ –∑–∞–ø–∏—Ç–∞–º–∏
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —è–∫ "—Å–≤—ñ—Ç–ª–æ—Ñ–æ—Ä" –¥–ª—è –æ–±–º–µ–∂–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –∑–∞–¥–∞—á
    """

    def __init__(self, max_concurrent: int = 3, site_name: str = "default"):
        """
        Args:
            max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
            site_name: –ù–∞–∑–≤–∞ —Å–∞–π—Ç—É –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        """
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.max_concurrent = max_concurrent
        self.site_name = site_name
        self.active_tasks = 0
        self.total_tasks = 0
        self.waiting_tasks = 0
        self.start_time = None

    async def acquire(self, task_name: str = "") -> bool:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ –¥–æ–∑–≤—ñ–ª –Ω–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è (–∑–∞–π—Ç–∏ –Ω–∞ "–∑–µ–ª–µ–Ω–µ —Å–≤—ñ—Ç–ª–æ")

        Args:
            task_name: –ù–∞–∑–≤–∞ –∑–∞–¥–∞—á—ñ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è

        Returns:
            bool: True —è–∫—â–æ –¥–æ–∑–≤—ñ–ª –æ—Ç—Ä–∏–º–∞–Ω–æ
        """
        self.waiting_tasks += 1
        logger.debug(f"[{self.site_name}] ‚è≥ –ó–∞–¥–∞—á–∞ '{task_name}' —á–µ–∫–∞—î... (–≤ —á–µ—Ä–∑—ñ: {self.waiting_tasks})")

        # –ß–µ–∫–∞—î–º–æ –Ω–∞ –∑–≤—ñ–ª—å–Ω–µ–Ω–Ω—è –º—ñ—Å—Ü—è
        await self.semaphore.acquire()

        self.waiting_tasks -= 1
        self.active_tasks += 1
        self.total_tasks += 1

        logger.debug(
            f"[{self.site_name}] ‚úÖ –ó–∞–¥–∞—á–∞ '{task_name}' —Å—Ç–∞—Ä—Ç—É—î (–∞–∫—Ç–∏–≤–Ω–∏—Ö: {self.active_tasks}/{self.max_concurrent})")
        return True

    def release(self, task_name: str = ""):
        """
        –ó–≤—ñ–ª—å–Ω–∏—Ç–∏ –¥–æ–∑–≤—ñ–ª (–≤–∏–π—Ç–∏ –∑ "–∑–µ–ª–µ–Ω–æ–≥–æ —Å–≤—ñ—Ç–ª–∞")

        Args:
            task_name: –ù–∞–∑–≤–∞ –∑–∞–¥–∞—á—ñ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        """
        self.active_tasks -= 1
        self.semaphore.release()
        logger.debug(
            f"[{self.site_name}] üîì –ó–∞–¥–∞—á–∞ '{task_name}' –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å (–∞–∫—Ç–∏–≤–Ω–∏—Ö: {self.active_tasks}/{self.max_concurrent})")

    async def run_with_semaphore(self, coro, task_name: str = ""):
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –∫–æ—Ä—É—Ç–∏–Ω—É –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º Semaphore (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ acquire/release)

        Args:
            coro: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
            task_name: –ù–∞–∑–≤–∞ –∑–∞–¥–∞—á—ñ

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –∫–æ—Ä—É—Ç–∏–Ω–∏
        """
        await self.acquire(task_name)
        try:
            if self.start_time is None:
                self.start_time = time.time()

            result = await coro
            return result
        finally:
            self.release(task_name)

    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ–±–æ—Ç–∏ Semaphore"""
        runtime = time.time() - self.start_time if self.start_time else 0
        return {
            'site': self.site_name,
            'max_concurrent': self.max_concurrent,
            'total_tasks': self.total_tasks,
            'current_active': self.active_tasks,
            'current_waiting': self.waiting_tasks,
            'runtime_seconds': round(runtime, 2)
        }

    def print_stats(self):
        """–í–∏–≤–æ–¥–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –ª–æ–≥–∏"""
        stats = self.get_stats()
        logger.info(f"üìä Semaphore —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {stats['site']}:")
        logger.info(f"   ‚Ä¢ –ú–∞–∫—Å. –æ–¥–Ω–æ—á–∞—Å–Ω–æ: {stats['max_concurrent']}")
        logger.info(f"   ‚Ä¢ –í—Å—å–æ–≥–æ –∑–∞–¥–∞—á: {stats['total_tasks']}")
        logger.info(f"   ‚Ä¢ –ß–∞—Å —Ä–æ–±–æ—Ç–∏: {stats['runtime_seconds']} —Å–µ–∫")


# –ì–ª–æ–±–∞–ª—å–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
_semaphores: Dict[str, SemaphoreManager] = {}


def get_semaphore(site_name: str, max_concurrent: int = 3) -> SemaphoreManager:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ Semaphore –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∞–π—Ç—É

    Args:
        site_name: –ù–∞–∑–≤–∞ —Å–∞–π—Ç—É
        max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤

    Returns:
        SemaphoreManager –¥–ª—è —Å–∞–π—Ç—É
    """
    if site_name not in _semaphores:
        _semaphores[site_name] = SemaphoreManager(max_concurrent, site_name)
        logger.info(f"üö¶ –°—Ç–≤–æ—Ä–µ–Ω–æ Semaphore –¥–ª—è {site_name} (–º–∞–∫—Å. {max_concurrent} –æ–¥–Ω–æ—á–∞—Å–Ω–æ)")
    return _semaphores[site_name]


class AsyncTaskGroup:
    """
    –ì—Ä—É–ø–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∏—Ö –∑–∞–¥–∞—á –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
    –î–æ–∑–≤–æ–ª—è—î –∑–∞–ø—É—Å–∫–∞—Ç–∏ –∫—ñ–ª—å–∫–∞ –∑–∞–¥–∞—á –∑ –æ–±–º–µ–∂–µ–Ω–Ω—è–º –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—Å—Ç—ñ
    """

    def __init__(self, max_concurrent: int = 3, name: str = "group"):
        """
        Args:
            max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –∑–∞–¥–∞—á
            name: –ù–∞–∑–≤–∞ –≥—Ä—É–ø–∏ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        """
        self.semaphore = get_semaphore(name, max_concurrent)
        self.tasks = []
        self.name = name

    def add_task(self, coro, task_name: str = ""):
        """
        –î–æ–¥–∞—Ç–∏ –∑–∞–¥–∞—á—É –¥–æ –≥—Ä—É–ø–∏

        Args:
            coro: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
            task_name: –ù–∞–∑–≤–∞ –∑–∞–¥–∞—á—ñ
        """
        self.tasks.append((coro, task_name))

    async def run_all(self) -> list:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –≤—Å—ñ –∑–∞–¥–∞—á—ñ –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º Semaphore

        Returns:
            list: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
        """
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –≥—Ä—É–ø–∏ '{self.name}' –∑ {len(self.tasks)} –∑–∞–¥–∞—á")

        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –≤—Å—ñ –∑–∞–¥–∞—á—ñ –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º
        results = []
        for i, (coro, task_name) in enumerate(self.tasks):
            result = await self.semaphore.run_with_semaphore(
                coro,
                task_name or f"task_{i + 1}"
            )
            results.append(result)

        self.semaphore.print_stats()
        return results

    async def run_parallel(self) -> list:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç–∏ –≤—Å—ñ –∑–∞–¥–∞—á—ñ –ü–ê–†–ê–õ–ï–õ–¨–ù–û –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º Semaphore

        Returns:
            list: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
        """
        logger.info(f"üöÄ –ü–∞—Ä–∞–ª–µ–ª—å–Ω–∏–π –∑–∞–ø—É—Å–∫ –≥—Ä—É–ø–∏ '{self.name}' –∑ {len(self.tasks)} –∑–∞–¥–∞—á")

        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–¥–∞—á—ñ –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º
        controlled_tasks = [
            self.semaphore.run_with_semaphore(coro, task_name)
            for coro, task_name in self.tasks
        ]

        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*controlled_tasks, return_exceptions=True)

        self.semaphore.print_stats()
        return results


# –ü—Ä–æ—Å—Ç–∏–π –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—é –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
def with_semaphore(site_name: str = "default", max_concurrent: int = 3):
    """
    –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ–±–º–µ–∂–µ–Ω–Ω—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–∏—Ö –≤–∏–∫–ª–∏–∫—ñ–≤ —Ñ—É–Ω–∫—Ü—ñ—ó

    Args:
        site_name: –ù–∞–∑–≤–∞ —Å–∞–π—Ç—É
        max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–¥–Ω–æ—á–∞—Å–Ω–∏—Ö –≤–∏–∫–ª–∏–∫—ñ–≤
    """
    semaphore = get_semaphore(site_name, max_concurrent)

    def decorator(func):
        async def wrapper(*args, **kwargs):
            async with semaphore.semaphore:
                return await func(*args, **kwargs)

        return wrapper

    return decorator

================================================================================
üìÑ –§–ê–ô–õ: src\settings.py
================================================================================

# src/settings.py
import yaml
from pathlib import Path
from loguru import logger
import sys

# ============================================
# –ë–ê–ó–û–í–Ü –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø
# ============================================

BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
LOG_DIR = DATA_DIR / "logs"
CONFIG_PATH = BASE_DIR / "config.yaml"

# –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –ø–∞–ø–∫–∏
DATA_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)


# ============================================
# –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–á
# ============================================

def load_config():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∑ YAML —Ñ–∞–π–ª—É"""
    if not CONFIG_PATH.exists():
        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {CONFIG_PATH} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
        return {}
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


config = load_config()


# ============================================
# –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –î–õ–Ø OLX
# ============================================

# URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º –Ω–æ—É—Ç–±—É–∫–∏)
START_URL = "https://www.olx.ua/uk/list/q-–Ω–æ—É—Ç–±—É–∫/"

# –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∑–±–æ—Ä—É
MAX_ITEMS = config.get("max_items", 100)

# –ó–∞—Ç—Ä–∏–º–∫–∏ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏ (—Å–µ–∫—É–Ω–¥–∏)
delays_cfg = config.get("delays", {"min": 4, "max": 8})
BASE_DELAY = (delays_cfg["min"], delays_cfg["max"])

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±—Ä–∞—É–∑–µ—Ä–∞
browser_cfg = config.get("browser", {})
HEADLESS = browser_cfg.get("headless", False)
TIMEOUT = browser_cfg.get("timeout", 60000)  # 60 —Å–µ–∫—É–Ω–¥

# User Agents –¥–ª—è –º–∞—Å–∫—É–≤–∞–Ω–Ω—è
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
]


# ============================================
# –ö–õ–ê–° –ü–û–ú–ò–õ–ö–ò –î–õ–Ø –ü–†–û–ö–°–Ü
# ============================================

class NoProxyAvailableError(Exception):
    """–í–∏–Ω—è—Ç–æ–∫, –∫–æ–ª–∏ –Ω–µ–º–∞—î –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –ø—Ä–æ–∫—Å—ñ"""
    pass


# ============================================
# –í–ê–õ–Ü–î–ê–¶–Ü–Ø –ü–†–û–ö–°–Ü
# ============================================

def validate_proxy(proxy):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î —á–∏ –ø—Ä–æ–∫—Å—ñ –º–∞—î –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç"""
    if not isinstance(proxy, dict):
        return False
    if "server" not in proxy:
        return False
    if not proxy["server"].startswith(("http://", "https://")):
        return False
    try:
        address = proxy["server"].split("://")[1]
        if ":" not in address:
            return False
        port = int(address.split(":")[1])
        if not (1 <= port <= 65535):
            return False
    except:
        return False

    if "username" in proxy and not isinstance(proxy["username"], str):
        return False
    if "password" in proxy and not isinstance(proxy["password"], str):
        return False

    return True


# ============================================
# –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ï –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ü–†–û–ö–°–Ü
# ============================================

from src.proxy_fetcher import WebshareProxyFetcher

logger.info("üîÑ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ –∑ Webshare API...")

try:
    fetcher = WebshareProxyFetcher()
    auto_proxies = fetcher.fetch_all_proxies()

    if auto_proxies:
        RAW_PROXY_LIST = auto_proxies
        logger.success(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(auto_proxies)} –ø—Ä–æ–∫—Å—ñ")
    else:
        logger.warning("‚ö†Ô∏è API –Ω–µ –ø–æ–≤–µ—Ä–Ω—É–≤ –ø—Ä–æ–∫—Å—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é —Ä–µ–∑–µ—Ä–≤–Ω–∏–π —Å–ø–∏—Å–æ–∫")
        RAW_PROXY_LIST = config.get("proxies", [])
except Exception as e:
    logger.warning(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –ø—Ä–æ–∫—Å—ñ: {e}")
    RAW_PROXY_LIST = config.get("proxies", [])


# ============================================
# –§–Ü–õ–¨–¢–†–ê–¶–Ü–Ø –ü–†–û–ö–°–Ü
# ============================================

VALID_PROXY_LIST = []
INVALID_PROXY_LIST = []

for proxy in RAW_PROXY_LIST:
    if validate_proxy(proxy):
        VALID_PROXY_LIST.append(proxy)
    else:
        INVALID_PROXY_LIST.append(proxy)


# ============================================
# –ü–ï–†–ï–í–Ü–†–ö–ê –ù–ê–Ø–í–ù–û–°–¢–Ü –ü–†–û–ö–°–Ü
# ============================================

def validate_proxy_list():
    """–ñ–æ—Ä—Å—Ç–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –ø—Ä–æ–∫—Å—ñ"""
    if not VALID_PROXY_LIST:
        logger.critical("=" * 60)
        logger.critical("üî¥ –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: –ù–ï–ú–ê–Ñ –î–û–°–¢–£–ü–ù–ò–• –ü–†–û–ö–°–Ü!")
        logger.critical("=" * 60)
        logger.critical("üõ°Ô∏è –ó–ê–•–ò–°–¢: –ü—Ä–æ–≥—Ä–∞–º–∞ –∑—É–ø–∏–Ω–µ–Ω–∞ - —Ä–æ–±–æ—Ç–∞ –±–µ–∑ –ø—Ä–æ–∫—Å—ñ –ó–ê–ë–û–†–û–ù–ï–ù–ê!")
        logger.critical("=" * 60)
        raise NoProxyAvailableError("–ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –ø—Ä–æ–∫—Å—ñ –¥–ª—è —Ä–æ–±–æ—Ç–∏")
    return VALID_PROXY_LIST


# –í–∏–∫–æ–Ω—É—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É
try:
    VALID_PROXY_LIST = validate_proxy_list()
    logger.info(f"üìä –î–æ—Å—Ç—É–ø–Ω–æ {len(VALID_PROXY_LIST)} —Ä–æ–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å—ñ")
except NoProxyAvailableError:
    sys.exit(1)

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Ä–æ—Ç–∞—Ü—ñ—ó
USE_PROXY_ROTATION = len(VALID_PROXY_LIST) > 1
if USE_PROXY_ROTATION:
    logger.info(f"üîÑ –£–≤—ñ–º–∫–Ω–µ–Ω–æ —Ä–æ—Ç–∞—Ü—ñ—é {len(VALID_PROXY_LIST)} –ø—Ä–æ–∫—Å—ñ")


# ============================================
# –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –°–ü–ò–°–ö–£ –ü–†–û–ö–°–Ü (–¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É)
# ============================================

if VALID_PROXY_LIST:
    PROXY_CACHE_FILE = DATA_DIR / "valid_proxies.json"
    import json
    with open(PROXY_CACHE_FILE, "w", encoding="utf-8") as f:
        safe_proxies = [{"server": p["server"]} for p in VALID_PROXY_LIST]
        json.dump(safe_proxies, f, indent=2)
    logger.debug(f"üíæ –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ {PROXY_CACHE_FILE}")


# ============================================
# –§–Ü–ù–ê–õ–¨–ù–Ü –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø
# ============================================

# –î–ª—è –∑—Ä—É—á–Ω–æ—Å—Ç—ñ –≤ –∫–æ–¥—ñ
PROXY_SETTINGS = VALID_PROXY_LIST[0]  # –ü–µ—Ä—à–∏–π –ø—Ä–æ–∫—Å—ñ –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É

logger.info("=" * 50)
logger.info(f"üöÄ –ü—Ä–æ–µ–∫—Ç –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ –¥–ª—è OLX")
logger.info(f"üìå URL: {START_URL}")
logger.info(f"üìä –ú–∞–∫—Å. –µ–ª–µ–º–µ–Ω—Ç—ñ–≤: {MAX_ITEMS}")
logger.info(f"üîå –ü—Ä–æ–∫—Å—ñ: {len(VALID_PROXY_LIST)} —à—Ç.")
logger.info("=" * 50)

================================================================================
üìÑ –§–ê–ô–õ: src\state_manager.py
================================================================================

# This class is responsible for saving and loading scraper checkpoints
import json
from pathlib import Path
from loguru import logger


class StateManager:
    def __init__(self, file_path: str = "data/checkpoint.json"):
        self.file_path = Path(file_path)
        self.file_path.parent.mkdir(parents=True, exist_ok=True)

    def save_checkpoint(self, url: str, gathered_count: int = 0):
        """Save current scraping progress to a file"""
        state = {
            "last_url": url,
            "count": gathered_count
        }
        with open(self.file_path, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=4)
        logger.debug(f"üíæ Checkpoint saved: {url}")

    def load_checkpoint(self) -> dict | None:
        """Load saved progress if it exists"""
        if self.file_path.exists():
            try:
                with open(self.file_path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error reading state file: {e}")
        return None

    def clear_checkpoint(self):
        """Remove checkpoint file after successful completion"""
        if self.file_path.exists():
            self.file_path.unlink()
            logger.info("üßπ Checkpoint removed (scraping completed).")

================================================================================
üìÑ –§–ê–ô–õ: src\stealth.py
================================================================================

# src/stealth.py - –§–Ü–ù–ê–õ–¨–ù–ê –í–ï–†–°–Ü–Ø –ó –ï–ö–°–ü–û–†–¢–û–ú
"""
–ú–æ–¥—É–ª—å —Ä—É—á–Ω–æ–≥–æ —Å—Ç–µ–ª—Å—É –¥–ª—è Playwright –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
"""

import random
import asyncio
from loguru import logger
from src.stealth_config import StealthConfig, USA_CONFIG, UKRAINE_CONFIG, GOOGLE_CONFIG, LINKEDIN_CONFIG


class ManualStealth:
    """
    –ö–ª–∞—Å –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –∑ –∫–æ–Ω—Ñ—ñ–≥–æ–º
    """

    def __init__(self, config: StealthConfig):
        self.config = config
        logger.debug(f"üìã –ö–æ–Ω—Ñ—ñ–≥: {config.timezone}")

    async def create_context(self, browser):
        """–°—Ç–≤–æ—Ä—é—î –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è–º–∏"""
        context = await browser.new_context(
            locale=self.config.languages[0],
            timezone_id=self.config.timezone,
            viewport={
                'width': self.config.screen_size[0],
                'height': self.config.screen_size[1]
            },
            extra_http_headers={
                'Accept-Language': ', '.join(self.config.languages),
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            }
        )
        logger.debug(f"üì¶ –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ –∑ locale: {self.config.languages[0]}")
        return context

    async def apply_to_page(self, page):
        """–î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è"""
        await page.add_init_script("""
            // WebGL –º–∞—Å–∫—É–≤–∞–Ω–Ω—è
            const getParameter = WebGLRenderingContext.prototype.getParameter;
            WebGLRenderingContext.prototype.getParameter = function(parameter) {
                if (parameter === 37445) return 'Intel Inc.';
                if (parameter === 37446) return 'Intel Iris OpenGL Engine';
                return getParameter.call(this, parameter);
            };

            // –•–æ–≤–∞—î–º–æ webdriver
            Object.defineProperty(Object.getPrototypeOf(navigator), 'webdriver', {
                get: () => undefined
            });
        """)
        logger.debug("üïµÔ∏è –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–æ")

    async def check_languages(self, page):
        """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ñ –º–æ–≤–∏"""
        langs = await page.evaluate("navigator.languages")
        lang = await page.evaluate("navigator.language")
        timezone = await page.evaluate("Intl.DateTimeFormat().resolvedOptions().timeZone")

        logger.info(f"üîç navigator.languages: {langs}")
        logger.info(f"üîç navigator.language: {lang}")
        logger.info(f"üîç –ß–∞—Å–æ–≤–∏–π –ø–æ—è—Å: {timezone}")

        return langs


# –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç–µ–ª—Å—É –ø—ñ–¥ —Å–∞–π—Ç
def get_stealth_for_site(site: str) -> ManualStealth:
    """–ü–æ–≤–µ—Ä—Ç–∞—î –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π —Å—Ç–µ–ª—Å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∞–π—Ç—É"""
    configs = {
        'amazon': USA_CONFIG,
        'ebay': USA_CONFIG,
        'google': GOOGLE_CONFIG,
        'linkedin': LINKEDIN_CONFIG,
        'ukraine': UKRAINE_CONFIG,
    }
    config = configs.get(site, USA_CONFIG)
    logger.debug(f"üéØ –°—Ç–≤–æ—Ä–µ–Ω–æ —Å—Ç–µ–ª—Å –¥–ª—è —Å–∞–π—Ç—É: {site}")
    return ManualStealth(config)


# –ï–ö–°–ü–û–†–¢–£–Ñ–ú–û –í–°–ï, –©–û –ü–û–¢–†–Ü–ë–ù–û
__all__ = ['ManualStealth', 'get_stealth_for_site']

================================================================================
üìÑ –§–ê–ô–õ: src\stealth_config.py
================================================================================

# src/stealth_config.py
"""
–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —Å—Ç–µ–ª—Å—É –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å–∞–π—Ç—ñ–≤
"""

from dataclasses import dataclass
from typing import List, Optional
import random


@dataclass
class StealthConfig:
    """
    –ö–ª–∞—Å –¥–ª—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ —Å—Ç–µ–ª—Å—É
    """

    # –ß–∞—Å–æ–≤–∏–π –ø–æ—è—Å
    timezone: str = 'Europe/Kiev'

    # –ú–æ–≤–∏ –±—Ä–∞—É–∑–µ—Ä–∞
    languages: List[str] = None

    # –†–æ–∑–º—ñ—Ä –µ–∫—Ä–∞–Ω—É
    screen_size: tuple = (1920, 1080)

    # WebGL –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
    webgl_vendor: str = 'Intel Inc.'
    webgl_renderer: str = 'Intel Iris OpenGL Engine'

    def __post_init__(self):
        if self.languages is None:
            self.languages = ['uk-UA', 'uk', 'en-US', 'en', 'ru']


# === –ö–û–ù–§–Ü–ì–ò –î–õ–Ø –†–Ü–ó–ù–ò–• –°–ê–ô–¢–Ü–í ===

UKRAINE_CONFIG = StealthConfig(
    timezone='Europe/Kiev',
    languages=['uk-UA', 'uk', 'ru', 'en-US', 'en'],
    screen_size=(1366, 768)
)

USA_CONFIG = StealthConfig(
    timezone='America/New_York',
    languages=['en-US', 'en'],
    screen_size=(1920, 1080)
)

GOOGLE_CONFIG = StealthConfig(
    timezone='America/Los_Angeles',
    languages=['en-US', 'en'],
    screen_size=(1536, 864)
)

LINKEDIN_CONFIG = StealthConfig(
    timezone='America/New_York',
    languages=['en-US', 'en'],
    screen_size=(1920, 1080)
)

TEST_CONFIG = StealthConfig(
    timezone='Europe/Kiev',
    languages=['uk-UA', 'uk', 'en-US', 'en'],
    screen_size=(1920, 1080)
)

================================================================================
üìÑ –§–ê–ô–õ: src\utils.py
================================================================================

import random
import asyncio
from loguru import logger


async def human_delay(min_sec=1, max_sec=3):
    """Asynchronously simulate human thinking delay."""
    sleep_time = random.uniform(min_sec, max_sec)
    await asyncio.sleep(sleep_time)


async def smooth_scroll(page):
    """
    Perform smooth scrolling with a "re-reading" effect
    (occasionally scrolling slightly upward).
    """
    try:
        total_height = await page.evaluate("document.body.scrollHeight")
        current_scroll = 0

        logger.debug("üìú Starting realistic scrolling...")

        while current_scroll < total_height:
            # Determine scroll step (downwards)
            step = random.randint(400, 800)
            current_scroll += step
            await page.mouse.wheel(0, step)
            await asyncio.sleep(random.uniform(0.4, 0.9))

            # --- RE-READING EFFECT ---
            # With a 15% probability, scroll slightly back up
            if random.random() < 0.15 and current_scroll > 1000:
                back_step = random.randint(-400, -200)
                current_scroll += back_step
                await page.mouse.wheel(0, back_step)
                logger.debug("üëÄ Scrolled slightly upward (re-reading effect)")
                await asyncio.sleep(random.uniform(1.0, 2.0))  # "Reading" pause

            # Update page height (for dynamically loaded pages)
            total_height = await page.evaluate("document.body.scrollHeight")

            if current_scroll > 15000:  # Safety guard for infinite pages
                break

    except Exception as e:
        logger.error(f"‚ö†Ô∏è Scrolling error: {e}")


async def human_mouse_move(page):
    """Simulate complex mouse movements with variable speed."""
    try:
        viewport = page.viewport_size or {'width': 1280, 'height': 720}

        for _ in range(random.randint(2, 4)):
            x = random.randint(50, viewport['width'] - 50)
            y = random.randint(50, viewport['height'] - 50)

            # steps=30‚Äì60 makes the movement slow and slightly shaky
            await page.mouse.move(x, y, steps=random.randint(30, 60))
            await asyncio.sleep(random.uniform(0.2, 0.6))

    except Exception as e:
        logger.debug(f"Mouse interaction skipped: {e}")

================================================================================
üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
================================================================================
üìÅ –ü–∞–ø–æ–∫ –∑ –∫–æ–¥–æ–º: 3
üìÑ –í—Å—å–æ–≥–æ —Ñ–∞–π–ª—ñ–≤: 23

üìä –ü–æ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è—Ö:
  (–±–µ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è): 3 —Ñ–∞–π–ª—ñ–≤
  .md: 1 —Ñ–∞–π–ª—ñ–≤
  .py: 17 —Ñ–∞–π–ª—ñ–≤
  .txt: 1 —Ñ–∞–π–ª—ñ–≤
  .yaml: 1 —Ñ–∞–π–ª—ñ–≤

üìä –ü–æ –ø–∞–ø–∫–∞—Ö:
  üìÅ .idea: 1 —Ñ–∞–π–ª—ñ–≤
  üìÅ .venv: 1 —Ñ–∞–π–ª—ñ–≤
  üìÅ pro_scraper_stealth_project3: 6 —Ñ–∞–π–ª—ñ–≤
  üìÅ src: 15 —Ñ–∞–π–ª—ñ–≤

üîí .gitignore: –Ñ
================================================================================
