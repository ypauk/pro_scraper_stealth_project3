# src/scraper.py
"""
–û—Å–Ω–æ–≤–Ω–∏–π —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è Rozetka
"""

import asyncio
import random
import time
from src.client import BrowserClient
from src.parser import RozetkaParser
from src.models import RozetkaItem
from loguru import logger
from src.utils import human_delay, smooth_scroll, human_mouse_move
from src.settings import BASE_DELAY, VALID_PROXY_LIST
from src.state_manager import StateManager
from src.exporter import Exporter
from src.stealth import ManualStealth, get_stealth_for_site
from src.proxy_monitor import ProxyMonitor
from src.semaphore_manager import get_semaphore


class Scraper:
    def __init__(self,
                 max_items: int = 50,
                 proxy: dict = None,
                 stealth: ManualStealth = None,
                 site_name: str = "Rozetka",
                 max_concurrent: int = 2,
                 discount_only: bool = False,
                 min_price: int = None,
                 max_price: int = None,
                 min_rating: float = None):

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ–∫—Å—ñ
        if proxy is None and not VALID_PROXY_LIST:
            logger.critical("=" * 60)
            logger.critical("üî¥ –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: –°–ö–†–ê–ü–ï–† –ù–ï –ú–û–ñ–ï –ü–†–ê–¶–Æ–í–ê–¢–ò –ë–ï–ó –ü–†–û–ö–°–Ü!")
            logger.critical("=" * 60)
            logger.critical("üõ°Ô∏è –ó–ê–•–ò–°–¢: –ü—Ä–æ–≥—Ä–∞–º–∞ –±—É–¥–µ –∑—É–ø–∏–Ω–µ–Ω–∞")
            logger.critical("üí° –†—ñ—à–µ–Ω–Ω—è: –î–æ–¥–∞–π—Ç–µ –ø—Ä–æ–∫—Å—ñ –≤ config.yaml")
            logger.critical("=" * 60)
            raise Exception("Scraper –≤–∏–º–∞–≥–∞—î –ø—Ä–æ–∫—Å—ñ –¥–ª—è —Ä–æ–±–æ—Ç–∏")

        self.client = BrowserClient(proxy=proxy)
        self.parser = RozetkaParser()
        self.max_items = max_items
        self.results: list[RozetkaItem] = []
        self._lock = asyncio.Lock()
        self.state_manager = StateManager()
        self.stealth = stealth or get_stealth_for_site('ukraine')

        # –§—ñ–ª—å—Ç—Ä–∏
        self.discount_only = discount_only
        self.min_price = min_price
        self.max_price = max_price
        self.min_rating = min_rating

        # –ö–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        self.site_name = site_name
        self.max_concurrent = max_concurrent
        self.semaphore = get_semaphore(site_name, max_concurrent)
        self.page_semaphore = asyncio.Semaphore(max_concurrent)

        # –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥
        self.proxy_monitor = ProxyMonitor()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stealth_used = 0
        self.behavior_imitated = 0
        self.pages_processed = 0
        self.start_time = None
        self.total_pages = 0
        self.failed_pages = 0
        self.filtered_items = 0

    def _apply_filters(self, item: RozetkaItem) -> bool:
        """
        –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —Ñ—ñ–ª—å—Ç—Ä–∏ –¥–æ —Ç–æ–≤–∞—Ä—É

        Returns:
            bool: True —è–∫—â–æ —Ç–æ–≤–∞—Ä –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –≤—Å—ñ —Ñ—ñ–ª—å—Ç—Ä–∏
        """
        # –§—ñ–ª—å—Ç—Ä –ø–æ –∑–Ω–∏–∂—Ü—ñ
        if self.discount_only and not item.has_discount:
            self.filtered_items += 1
            return False

        # –§—ñ–ª—å—Ç—Ä –ø–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ–π —Ü—ñ–Ω—ñ
        if self.min_price and item.price_value < self.min_price:
            self.filtered_items += 1
            return False

        # –§—ñ–ª—å—Ç—Ä –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ñ–π —Ü—ñ–Ω—ñ
        if self.max_price and item.price_value > self.max_price:
            self.filtered_items += 1
            return False

        # –§—ñ–ª—å—Ç—Ä –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É
        if self.min_rating and (item.rating is None or item.rating < self.min_rating):
            self.filtered_items += 1
            return False

        return True

    async def _simulate_human_behavior(self, page):
        """–Ü–º—ñ—Ç–∞—Ü—ñ—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –ª—é–¥–∏–Ω–∏"""
        try:
            # –í–∏–ø–∞–¥–∫–æ–≤–∞ –ø—Ä–æ–∫—Ä—É—Ç–∫–∞
            scroll_amount = random.randint(200, 500)
            await page.evaluate(f"window.scrollBy(0, {scroll_amount})")
            await asyncio.sleep(random.uniform(0.5, 1.5))

            # –í–∏–ø–∞–¥–∫–æ–≤–∏–π —Ä—É—Ö –º–∏—à—ñ
            viewport = page.viewport_size
            if viewport:
                x = random.randint(100, viewport['width'] - 100)
                y = random.randint(100, viewport['height'] - 100)
                await page.mouse.move(x, y, steps=random.randint(10, 20))

                # –Ü–Ω–æ–¥—ñ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ —Ä—É—Ö–∏
                if random.random() < 0.3:
                    for _ in range(random.randint(2, 4)):
                        new_x = x + random.randint(-50, 50)
                        new_y = y + random.randint(-50, 50)
                        await page.mouse.move(new_x, new_y, steps=5)
                        await asyncio.sleep(random.uniform(0.1, 0.2))

            logger.debug(f"[{self.site_name}] üñ±Ô∏è –Ü–º—ñ—Ç–∞—Ü—ñ—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –ª—é–¥–∏–Ω–∏")
            return True
        except Exception as e:
            logger.debug(f"[{self.site_name}] –ù–µ –≤–¥–∞–ª–æ—Å—è —ñ–º—ñ—Ç—É–≤–∞—Ç–∏ –ø–æ–≤–µ–¥—ñ–Ω–∫—É: {e}")
            return False

    async def scrape_page(self, url: str, index: int, proxy_override: dict = None) -> str | None:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –æ–¥–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É –∑ –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —á–µ—Ä–µ–∑ Semaphore
        """
        if len(self.results) >= self.max_items:
            logger.info(f"[{self.site_name}] üéØ –î–æ—Å—è–≥–Ω—É—Ç–æ –ª—ñ–º—ñ—Ç—É –≤ {self.max_items} —Ç–æ–≤–∞—Ä—ñ–≤")
            return None

        async with self.page_semaphore:
            logger.debug(
                f"[{self.site_name}] üîë –û—Ç—Ä–∏–º–∞–Ω–æ –¥–æ—Å—Ç—É–ø –¥–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ #{index} "
                f"(–∞–∫—Ç–∏–≤–Ω–∏—Ö: {self.max_concurrent - self.page_semaphore._value}/{self.max_concurrent})"
            )
            return await self._scrape_page_internal(url, index, proxy_override)

    async def _scrape_page_internal(self, url: str, index: int, proxy_override: dict = None) -> str | None:
        """–í–Ω—É—Ç—Ä—ñ—à–Ω—ñ–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏"""

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø—Ä–æ–∫—Å—ñ
        if not VALID_PROXY_LIST:
            logger.critical(f"[{self.site_name}] üî¥ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ù–ï–ú–ê–Ñ –ü–†–û–ö–°–Ü!")
            return "ERROR_SIGNAL"

        # –í–∏–±—ñ—Ä –ø—Ä–æ–∫—Å—ñ
        safe_index = (index - 1) % len(VALID_PROXY_LIST)
        current_proxy = proxy_override or VALID_PROXY_LIST[safe_index]
        logger.info(f"[{self.site_name}] üîå [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ü—Ä–æ–∫—Å—ñ: {current_proxy['server']}")

        # –í–∏–ø–∞–¥–∫–æ–≤–∏–π User-Agent
        current_ua = self.client.get_random_ua()

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        try:
            if self.stealth:
                context = await self.stealth.create_context(self.client.browser)
                logger.debug(f"[{self.site_name}] üïµÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–≤–æ—Ä–µ–Ω–æ —á–µ—Ä–µ–∑ —Å—Ç–µ–ª—Å")
            else:
                context = await self.client.browser.new_context(
                    user_agent=current_ua,
                    proxy=current_proxy,
                    viewport={
                        "width": random.choice([1366, 1440, 1536, 1920]),
                        "height": random.choice([768, 900, 864, 1080])
                    }
                )
        except Exception as e:
            logger.error(f"[{self.site_name}] ‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É: {e}")
            return "ERROR_SIGNAL"

        page = await context.new_page()

        # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —Å—Ç–µ–ª—Å—É
        if self.stealth:
            try:
                await self.stealth.apply_to_page(page)
                self.stealth_used += 1
                logger.debug(f"[{self.site_name}] üïµÔ∏è –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–∞—Å–∫—É–≤–∞–Ω–Ω—è –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–æ")
            except Exception as e:
                logger.warning(f"[{self.site_name}] ‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —Å—Ç–µ–ª—Å—É: {e}")

        try:
            logger.info(f"[{self.site_name}] üöÄ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {url}")

            # –Ü–º—ñ—Ç–∞—Ü—ñ—è –ø–æ–≤–µ–¥—ñ–Ω–∫–∏ –ª—é–¥–∏–Ω–∏
            if random.random() < 0.7:
                if await self._simulate_human_behavior(page):
                    self.behavior_imitated += 1

            # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É
            await page.goto(url, wait_until="domcontentloaded", timeout=60000)

            # –ü–∞—Ä—Å–∏–º–æ —Ç–æ–≤–∞—Ä–∏
            new_items = await self.parser.parse_listings(page)
            next_page_url = await self.parser.get_next_page(page)

            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç–∞ –æ–Ω–æ–≤–ª—é—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            added_count = 0
            for item in new_items:
                if self._apply_filters(item):
                    async with self._lock:
                        if len(self.results) < self.max_items:
                            self.results.append(item)
                            added_count += 1

                            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º—É —á–∞—Å—ñ
                            Exporter.append_to_csv(item, filename=f"{self.site_name.lower()}_live.csv")
                # else: –≤–∂–µ –ø–æ—Ä–∞—Ö–æ–≤–∞–Ω–æ –≤ _apply_filters

            # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            count = len(self.results)

            if added_count > 0:
                logger.success(
                    f"[{self.site_name}] ‚úÖ [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] +{added_count} —Ç–æ–≤–∞—Ä—ñ–≤ "
                    f"(–í—Å—å–æ–≥–æ: {count}/{self.max_items})"
                )
                self.total_pages += 1
            else:
                logger.info(
                    f"[{self.site_name}] ‚ÑπÔ∏è [–°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index}] –ù–æ–≤–∏—Ö —Ç–æ–≤–∞—Ä—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ "
                    f"(–í—Å—å–æ–≥–æ: {count}/{self.max_items})"
                )

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
            if next_page_url and next_page_url != url:  # –ù–µ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–ª—è infinite scroll
                self.state_manager.save_checkpoint(next_page_url, count)

            # –í–∏–ø–∞–¥–∫–æ–≤–∞ –∑–∞—Ç—Ä–∏–º–∫–∞
            delay = random.uniform(BASE_DELAY[0], BASE_DELAY[1])
            logger.debug(f"[{self.site_name}] üí§ –ó–∞—Ç—Ä–∏–º–∫–∞ {delay:.1f}—Å –ø–µ—Ä–µ–¥ –Ω–∞—Å—Ç—É–ø–Ω–æ—é —Å—Ç–æ—Ä—ñ–Ω–∫–æ—é")
            await asyncio.sleep(delay)

            return next_page_url

        except Exception as e:
            logger.error(f"[{self.site_name}] ‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ #{index}: {e}")
            self.failed_pages += 1
            return "ERROR_SIGNAL"
        finally:
            await context.close()
            self.pages_processed += 1

    async def scrape_page_with_retry(self, url: str, index: int, max_retries: int = 3):
        """–°–ø—Ä–æ–±–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫—É –∑ —Ä–æ—Ç–∞—Ü—ñ—î—é –ø—Ä–æ–∫—Å—ñ"""

        if not self.proxy_monitor or not self.proxy_monitor.check_proxy_health():
            logger.critical(f"[{self.site_name}] üî¥ –í–°–Ü –ü–†–û–ö–°–Ü –ú–ï–†–¢–í–Ü!")
            return "ERROR_SIGNAL"

        for attempt in range(max_retries):
            start_time = time.time()
            current_proxy = None

            try:
                if not VALID_PROXY_LIST:
                    logger.critical(f"[{self.site_name}] üî¥ –ü–†–û–ö–°–Ü –ó–ê–ö–Ü–ù–ß–ò–õ–ò–°–¨!")
                    if self.proxy_monitor:
                        self.proxy_monitor.print_stats()
                    return "ERROR_SIGNAL"

                proxy_index = (index + attempt) % len(VALID_PROXY_LIST)
                current_proxy = VALID_PROXY_LIST[proxy_index]

                logger.info(f"[{self.site_name}] üîÑ –°–ø—Ä–æ–±–∞ {attempt + 1}/{max_retries} –¥–ª—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ #{index}")
                logger.info(f"[{self.site_name}] üîå –ü—Ä–æ–∫—Å—ñ: {current_proxy['server']}")

                result = await asyncio.wait_for(
                    self.scrape_page(url, index, current_proxy),
                    timeout=60
                )

                response_time = time.time() - start_time
                if self.proxy_monitor and current_proxy:
                    self.proxy_monitor.log_proxy_usage(
                        current_proxy['server'],
                        success=True,
                        response_time=response_time
                    )

                if result != "ERROR_SIGNAL":
                    if attempt > 0:
                        logger.success(
                            f"[{self.site_name}] ‚úÖ –°—Ç–æ—Ä—ñ–Ω–∫–∞ #{index} –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∑ {attempt + 1} —Å–ø—Ä–æ–±–∏"
                        )
                    return result

            except asyncio.TimeoutError:
                logger.warning(f"[{self.site_name}] ‚è∞ –¢–∞–π–º–∞—É—Ç –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ #{index}")
                if self.proxy_monitor and current_proxy:
                    self.proxy_monitor.log_proxy_usage(current_proxy['server'], success=False)

            except Exception as e:
                if self.proxy_monitor and current_proxy:
                    self.proxy_monitor.log_proxy_usage(current_proxy['server'], success=False)

                if "ERR_PROXY_CONNECTION_FAILED" in str(e) and current_proxy:
                    logger.warning(f"[{self.site_name}] üî¥ –ü—Ä–æ–∫—Å—ñ –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î: {current_proxy['server']}")
                    if VALID_PROXY_LIST and current_proxy in VALID_PROXY_LIST:
                        VALID_PROXY_LIST.remove(current_proxy)
                        logger.warning(
                            f"[{self.site_name}] üóëÔ∏è –í–∏–¥–∞–ª–µ–Ω–æ –º–µ—Ä—Ç–≤–µ –ø—Ä–æ–∫—Å—ñ. "
                            f"–ó–∞–ª–∏—à–∏–ª–æ—Å—å: {len(VALID_PROXY_LIST)}"
                        )

                        if not VALID_PROXY_LIST and self.proxy_monitor:
                            logger.critical(f"[{self.site_name}] üî¥ –í–°–Ü –ü–†–û–ö–°–Ü –í–ò–î–ê–õ–ï–ù–û!")
                            self.proxy_monitor.print_stats()
                            return "ERROR_SIGNAL"
                else:
                    logger.warning(f"[{self.site_name}] ‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞: {str(e)[:100]}")

            wait_time = 2 ** attempt
            logger.info(f"[{self.site_name}] üí§ –û—á—ñ–∫—É–≤–∞–Ω–Ω—è {wait_time}—Å")
            await asyncio.sleep(wait_time)

        logger.error(f"[{self.site_name}] ‚ùå –í—Å—ñ {max_retries} —Å–ø—Ä–æ–± –¥–ª—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏ #{index} –Ω–µ –≤–¥–∞–ª–∏—Å—è")
        return "ERROR_SIGNAL"

    async def run(self, start_url: str):
        """–û—Å–Ω–æ–≤–Ω–∏–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫—É"""
        self.start_time = time.time()
        await self.client.start()

        checkpoint_data = self.state_manager.load_checkpoint()
        current_url = start_url

        if isinstance(checkpoint_data, dict):
            current_url = checkpoint_data.get("last_url", start_url)
        elif isinstance(checkpoint_data, str):
            current_url = checkpoint_data

        if current_url != start_url:
            logger.info(f"[{self.site_name}] ‚ôªÔ∏è –í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ —á–µ–∫–ø–æ—ó–Ω—Ç–∞: {current_url}")

        page_index = 1
        start_time = time.time()
        last_url = None

        try:
            while current_url and len(self.results) < self.max_items:
                # –ó–∞–ø–æ–±—ñ–≥–∞—î–º–æ –∑–∞—Ü–∏–∫–ª–µ–Ω–Ω—é
                if current_url == last_url:
                    logger.warning(f"[{self.site_name}] ‚ö†Ô∏è –í–∏—è–≤–ª–µ–Ω–æ –∑–∞—Ü–∏–∫–ª–µ–Ω–Ω—è, –ø–µ—Ä–µ—Ä–∏–≤–∞—é")
                    break

                last_url = current_url
                result = await self.scrape_page_with_retry(current_url, page_index)

                if result == "ERROR_SIGNAL":
                    logger.warning(f"[{self.site_name}] ‚ö†Ô∏è –ü–µ—Ä–µ—Ä–∏–≤–∞–Ω–Ω—è. –ß–µ–∫–ø–æ—ó–Ω—Ç: {current_url}")
                    break

                current_url = result
                page_index += 1

            # –§—ñ–Ω–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            elapsed_time = time.time() - start_time
            logger.info("=" * 70)
            logger.info(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–õ–Ø {self.site_name}")
            logger.info("=" * 70)
            logger.info(f"üìÑ –°—Ç–æ—Ä—ñ–Ω–æ–∫ —É—Å–ø—ñ—à–Ω–æ: {self.total_pages}")
            logger.info(f"‚ùå –°—Ç–æ—Ä—ñ–Ω–æ–∫ –∑ –ø–æ–º–∏–ª–∫–∞–º–∏: {self.failed_pages}")
            logger.info(f"üì¶ –¢–æ–≤–∞—Ä—ñ–≤ –∑—ñ–±—Ä–∞–Ω–æ: {len(self.results)}")
            logger.info(f"üîç –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {self.filtered_items}")
            logger.info(f"‚è±Ô∏è –ß–∞—Å: {elapsed_time:.1f} —Å–µ–∫")

            if elapsed_time > 0:
                speed = len(self.results) / elapsed_time
                logger.info(f"‚ö° –®–≤–∏–¥–∫—ñ—Å—Ç—å: {speed:.2f} —Ç–æ–≤–∞—Ä—ñ–≤/—Å–µ–∫")

            logger.info(f"üïµÔ∏è –°—Ç–µ–ª—Å: {self.stealth_used} —Ä–∞–∑—ñ–≤")
            logger.info(f"üñ±Ô∏è –Ü–º—ñ—Ç–∞—Ü—ñ–π: {self.behavior_imitated} —Ä–∞–∑—ñ–≤")
            logger.info("=" * 70)

            if self.proxy_monitor:
                self.proxy_monitor.print_stats()

            # –û—á–∏—â–∞—î–º–æ —á–µ–∫–ø–æ—ó–Ω—Ç –ø—Ä–∏ —É—Å–ø—ñ—à–Ω–æ–º—É –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ñ
            if len(self.results) >= self.max_items or current_url is None:
                self.state_manager.clear_checkpoint()

            return self.results

        except Exception as e:
            logger.critical(f"[{self.site_name}] üí• –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self.results
        finally:
            await self.client.stop()